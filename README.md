# Awesome-Multimodal-Large-Language-Models
his is a repository for organizing articles related to Multimodal Large Language Models, Large Language Models, and Diffusion Models; Most papers are linked to **my reading notes**. Feel free to visit my [personal homepage](https://yfzhang114.github.io/) and contact me for collaboration and discussion.


### About Me :high_brightness: 
I'm a final-year Ph.D. student at the State Key Laboratory of Pattern Recognition, the University of Chinese Academy of Sciences, advised by Prof. [Tieniu Tan](http://people.ucas.ac.cn/~tantieniu). I have also spent time at Microsoft, advised by Prof. [Jingdong Wang](https://jingdongwang2017.github.io/), alibaba DAMO Academy, work with Prof. [Rong Jin](https://scholar.google.com/citations?user=CS5uNscAAAAJ&hl=zh-CN).


###  ğŸ”¥ Updated 2025-12-04

- [2025-12-04] Updated with several recent representative multimodal reasoning methods (Qwen3-VL, Kimi K2.5, Deepseek-OCR v2) and think with images (Deepeyes v2, Skywork R1V4), along with their reading notes.
- We present [Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch](https://skywork-r1v4-lite.netlify.app/) [[Reading Notes]](https://zhuanlan.zhihu.com/p/1979848119471608282), Skywork-R1V4 requires only 30K SFT data and activates "think with image," search, planning, and interleaved image manipulation/search capabilities, with 3B activated parameters, outperforming Gemini 2.5 Flash on all perception and deep research benchmarks.
- We present [Thyme: Think Beyond Images](https://thyme-vl.github.io/) [[Reading Notes]](https://zhuanlan.zhihu.com/p/1942175827547649963), Thyme transcends traditional ``thinking with images'' paradigms by autonomously generating and executing diverse image processing and computational operations through executable code.
- We present [R1-Reward](https://github.com/yfzhang114/r1_reward) [[Reading Notes]](https://zhuanlan.zhihu.com/p/1903095194166997749), which is a comprehensive project focused on enhancing multimodal reward modeling through reinforcement learning.
- We present [MME-Unify](https://mme-unify.github.io/), a comprehensive benchmark for unified multimodal models (GPT-4o, Gemini-2-flash, Janus-Pro, EMU3, Show-o, VILA-U).
- We present [MM-RLHF](https://github.com/yfzhang114/MM-RLHF), a comprehensive dataset of 120K fully human-annotated preference data, along with a robust reward model and training algorithm, designed to enhance MLLM alignment and significantly improve performance across 27 benchmark tasks.
- Our benchmark  [MME-RealWorld](https://mme-realworld.github.io/) has been released, the most difficult and largest pure manual annotation image perception benchmark so far.  [[Code]](https://github.com/yfzhang114/MME-RealWorld) [[Reading Notes]](https://zhuanlan.zhihu.com/p/717129017)
- Our model  [SliME](https://arxiv.org/abs/2406.08487) has been released, a high-resolution MLLM that can also be extend to video analysis.  [[Code]](https://github.com/yfzhang114/SliME) [[Reading Notes]](https://zhuanlan.zhihu.com/p/703258020)
- Our paper  [Debiasing Multimodal Large Language Models](https://arxiv.org/abs/2403.05262) has been released.  [[Code]](https://github.com/yfzhang114/LLaVA-Align) [[Reading Notes]](https://zhuanlan.zhihu.com/p/686461442)

# Table of Contents (ongoing)
- [Awesome-Multimodal-Large-Language-Models](#awesome-multimodal-large-language-models)
- [Table of Contents (ongoing)](#table-of-contents-ongoing)
- [Survey and Outlook](#survey-and-outlook)
- [Multimodal Reasoning & Think with Images (o3)](#multimodal-reasoning-and-think-with-images-o3)
- [Multimodal Large Language Models](#multimodal-large-language-models)
- [BenchMark and Dataset](#benchmark-and-dataset)
- [Unify Multimodal Understanding and Generation](#unify-multimodal-understanding-and-generation)
- [Alignment With Human Preference (MLLM)](#alignment-with-human-preference-mllm)
- [Alignment With Human Preference (LLM)](#alignment-with-human-preference-llm)


# Survey and Outlook
1. [ä¸‡å­—é•¿æ–‡æ€»ç»“å¤šæ¨¡æ€å¤§æ¨¡å‹è¯„ä¼°æœ€æ–°è¿›å±•](https://zhuanlan.zhihu.com/p/16815782175)
2. [ä¸‡å­—é•¿æ–‡æ€»ç»“å¤šæ¨¡æ€å¤§æ¨¡å‹æœ€æ–°è¿›å±•ï¼ˆModality Bridgingç¯‡ï¼‰](https://zhuanlan.zhihu.com/p/688215018)
3. [ä¸‡å­—é•¿æ–‡æ€»ç»“å¤šæ¨¡æ€å¤§æ¨¡å‹æœ€æ–°è¿›å±•ï¼ˆVideoç¯‡ï¼‰](https://zhuanlan.zhihu.com/p/704246896)
4. [Aligning Large Language Models with Human](https://zhuanlan.zhihu.com/p/693160839)'
5. [ä¸‡å­—é•¿æ–‡æ¢³ç†RLæœ€æ–°è¿›å±•ï¼šä»policy gradientåˆ°REINFORCE++](https://zhuanlan.zhihu.com/p/24421624957)
6. [ä¸‡å­—é•¿æ–‡æ€»ç»“å¤šæ¨¡æ€å¤§æ¨¡å‹åè®­ç»ƒï¼šä»å¹»è§‰åˆ°o1-reasoning](https://zhuanlan.zhihu.com/p/31278114666)
7. [ä¸‡å­—é•¿æ–‡æ€»ç»“å¤šæ¨¡æ€å¤§æ¨¡å‹æœ€æ–°è¿›å±•(ç”Ÿæˆ-ç†è§£å¤§ä¸€ç»Ÿ)](https://zhuanlan.zhihu.com/p/719475102)
8. [A Survey of Reinforcement Learning for Large Reasoning Models](https://arxiv.org/pdf/2509.08827)
9. [The Landscape of Agentic Reinforcement Learning for LLMs: A Survey](https://arxiv.org/pdf/2509.02547)

# Multimodal Reasoning And Think With Images (o3)
1. (Think with images) [Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch](https://zhuanlan.zhihu.com/p/1979848119471608282)(30Kæ•°æ®SFTï¼Œæ— éœ€rlæ¿€æ´»think with imageã€‚planningï¼Œtext/image search.)
2. (Think with images) [DeepEyesV2: Toward Agentic Multimodal Model](https://zhuanlan.zhihu.com/p/1971564951412924453)(å›¾åƒæ“ä½œ+æœç´¢ï¼Œagentic mllm)
3. (Think with images) [Thyme: Think Beyond Images](https://zhuanlan.zhihu.com/p/1942175827547649963)(é€šè¿‡æ¨¡å‹è‡ªèº«codingå®Œæˆè£å‰ªï¼Œæ—‹è½¬ï¼Œå¯¹æ¯”åº¦å¢å¼ºï¼Œå¤æ‚è®¡ç®—ï¼Œagentic mllmçš„åˆæ­¥æ¢ç´¢)
4. (Think with images) [mini-o3: scaling up reasoning patterns and interaction turns for visual search](https://arxiv.org/abs/2509.00676)(é«˜åˆ†çš„æ•°æ®ï¼Œbenchmarkå’Œrlç®—æ³•)
5. (Think with images) [Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views](https://zhuanlan.zhihu.com/p/1968671990681403733)(3Dçš„think with image)
6. (Think with images) [Latent Visual Reasoning](https://zhuanlan.zhihu.com/p/1966113562880619373)(ç›´æ¥é¢„æµ‹vision tokenå®ç°Think with image)
7. (Reward Model) [basereward: a strong baseline for multimodal reward model](https://zhuanlan.zhihu.com/p/1955620828617085143)(å…³äºå¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹çš„ä¸€å †æœ‰ç”¨çš„trick)
8. (Reward Model) [R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning](https://zhuanlan.zhihu.com/p/1903095194166997749)(StableReinforceç®—æ³•boostå¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹å»ºæ¨¡)
9. (LLM) [Ministral 3](https://zhuanlan.zhihu.com/p/1994760575448810344)(Ministral 3: æè‡´çš„è’¸é¦å’Œåå¥½å¯¹é½)
10. (LLM) [Introducing MiMo-V2-Flash](https://zhuanlan.zhihu.com/p/1984919167934170020)(MiMo-V2-Flashé€å¸§å­¦ä¹ ç‰ˆ)
11. (LLM) [Deepseek-Math-v2](https://zhuanlan.zhihu.com/p/1977742909622211010)(ç»™reward modelè®­ç»ƒä¸€ä¸ªreward model)
12. (LLM)[ POLARIS-4B](https://zhuanlan.zhihu.com/p/1930601703209665224)(Seedï¼šå››å¤§æŠ€å·§åŠ©åŠ›RLç‚¼ä¸¹)
13. (LLM)[Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning](https://zhuanlan.zhihu.com/p/1913555493412115868)(ä»…ä½¿ç”¨20%çš„tokenè¿›è¡Œæ¨ç†è®­ç»ƒï¼Œæ•ˆæœç”šè‡³èƒ½è¶…è¶Šå…¨æ¢¯åº¦æ›´æ–°)
14. (LLM)[ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models](https://www.zhihu.com/people/yukio-2)(åªè¦è®­ç»ƒå¾—å¤Ÿä¹…ã€å¤Ÿç¨³å®šã€å¤Ÿå¤šæ ·åŒ–ï¼Œå¼ºåŒ–å­¦ä¹ å®Œå…¨å¯ä»¥å¸®åŠ©æ¨¡å‹å‘ç°å…¨æ–°çš„æ¨ç†ç­–ç•¥ï¼Œçªç ´åŸæœ‰çš„èƒ½åŠ›è¾¹ç•Œ)
15. (LLM)[Thinker: Learning to Think Fast and Slow](https://www.themoonlight.io/zh/review/thinker-learning-to-think-fast-and-slow)(å…ˆç»™å®štokené¢„ç®—fast thinkingï¼Œç­”æ¡ˆé”™è¯¯å†slow thinking)
16. (LLM)[Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback](https://www.chatpaper.ai/zh/dashboard/paper/5be0c41c-29dc-4818-a870-e6d16eabc0d2)(rule based rewardæ€§èƒ½ç“¶é¢ˆåé€šè¿‡LLMç”Ÿæˆcriticè¿˜èƒ½è¿›ä¸€æ­¥æå‡)
17. (LLM) [GLM 4.5](https://zhuanlan.zhihu.com/p/1947992148415873229)(GLM4.5æŠ€æœ¯è·¯çº¿ï¼šå¤šé˜¶æ®µrlæ˜¯agentic+reasoningçš„å…³é”®)
18. (MLLM) [Kimi K 2.5](https://zhuanlan.zhihu.com/p/2000719027690030326)(Kimi K2.5 æŠ€æœ¯æŠ¥å‘Šé˜…è¯»ç¬”è®°)
19. (MLLM) [Deepseek OCR V2](https://www.zhihu.com/question/1999468225642119587)(å¦‚ä½•è¯„ä»·DeepSeek-OCR-2 æ¨¡å‹ï¼Ÿ)
20. (MLLM) [Qwen3-VL](https://zhuanlan.zhihu.com/p/1977442723322679677)(ç»“æ„ä¸Šç”¨äº†deep stackï¼Œæ–°çš„mropeä¸Video Timestampï¼Œç®—æ³•ä¸Šç”¨äº†Distillationçš„trickï¼Œsapoç­‰)
21. (MLLM Reward) [BaseReward: A Strong Baseline for Multimodal Reward Model](https://zhuanlan.zhihu.com/p/1955620828617085143)(å…³äºå¤šæ¨¡æ€å¥–åŠ±æ¨¡å‹çš„ä¸€å †æœ‰ç”¨çš„trick)
22. (MLLM) [InternVL3.5](https://zhuanlan.zhihu.com/p/1943711475937031695)(InternVL3.5 æŠ€æœ¯æŠ¥å‘Šé€Ÿè§ˆ:ç¦»çº¿+åœ¨çº¿RLæœ‰è¯´æ³•)
23. (MLLM) [ERNIE 4.5 Technical Report](https://zhuanlan.zhihu.com/p/1923373773127655901)(æ–‡å¿ƒä¸€è¨€æŠ€æœ¯è·¯çº¿)
24. (MLLM) [longvila scaling long-context visual language models for long videos](https://zhuanlan.zhihu.com/p/1929152101721830183)(AIé•¿è§†é¢‘RLæ–°çªç ´ï¼šè‹±ä¼Ÿè¾¾å‘å¸ƒLongVILA)
25. (MLLM) [Skywork-R1V3 Technical Report](https://zhuanlan.zhihu.com/p/1928058102290310188)(Skywork-R1V3 Technical Reportè§£è¯»ï¼ŒMMMU 76)
26. (MLLM) [Kwai Keye-VL Technical Report](https://zhuanlan.zhihu.com/p/1924429130553857058)(å¿«æ‰‹keye-vlï¼ŒçŸ­è§†é¢‘ç†è§£ï¼Œauto thinkï¼Œthink with image)
27. (MLLM) [GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning](https://zhuanlan.zhihu.com/p/1924064703946158968)(æ™ºè°±å¤šæ¨¡æ€å¤§æ¨¡å‹GLM-4.1V-thiningæŠ€æœ¯è·¯çº¿)
28. (MLLM)[Seed1.5 VL](https://arxiv.org/abs/2505.07062)(æ··åˆrule-basedä¸ORM basedçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œsft-rlå¤šæ­¥è¿­ä»£)
29. (MLLM)[MiMo VL](https://www.arxiv.org/abs/2506.03569)(æ··åˆrule-basedä¸ORM basedçš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼Œ GRPO)
30. (MLLM)[SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis](https://www.chatpaper.ai/zh/dashboard/paper/c013bdbb-f0e8-41b3-a595-a836d14b68ce)(é€šè¿‡å¯éªŒè¯æ•°æ®(rulse baedæ•°æ®)åˆæˆæ‰©å±•è§†è§‰æ¨ç†èƒ½åŠ›)
31. (Think with images) [DeepEyes: Incentivizing "Thinking with Images" via Reinforcement Learning](https://zhuanlan.zhihu.com/p/1908543355161417216)(çº¯å¼ºåŒ–å­¦ä¹ æ¿€å‘think with imagesçš„èƒ½åŠ›)
32. (Agentic) [rStar2-Agent: Agentic Reasoning Technical Report](https://zhuanlan.zhihu.com/p/1947981998569260594)(GRPO-RoC:è½¨è¿¹è´¨é‡è¿‡æ»¤æ˜¯agentic RLçš„å…³é”®)


# Multimodal Large Language Models
1. (S-Lab) [From Pixels to Words -- Towards Native Vision-Language Primitives at Scale](https://zhuanlan.zhihu.com/p/1991924525370200332)(åŸç”Ÿå¤šæ¨¡æ€æ–°çªç ´ï¼š390M æ•°æ®è¶…è¶ŠInternVL3)
2. (Meta,Stanford) [Apollo: An Exploration of Video Understanding in Large Multimodal Models](https://zhuanlan.zhihu.com/p/13353334416)(ä»€ä¹ˆæ˜¯MLLMè§†é¢‘ç†è§£çš„å…³é”®å› ç´ )
3. (Shanghai AI Lab) [Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling](https://zhuanlan.zhihu.com/p/12309812997)(InternVL2.5æŠ€æœ¯ç»†èŠ‚-è®©å¼€æºå¤šæ¨¡æ€æ¨¡å‹å†è¿›ä¸€æ­¥)
4. (NVIDIA) [NVLM: Open Frontier-Class Multimodal LLMs](https://zhuanlan.zhihu.com/p/720542712)(ä¸‰ç§ä¸åŒçš„ç‰¹å¾èåˆæ¡†æ¶æ·±åº¦æ¢ç´¢)
5. (Allen Institute for AI) [Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models](https://zhuanlan.zhihu.com/p/720542712)(æœ¬æ–‡çš„æ”¹è¿›é›†ä¸­åœ¨æ•°æ®ä¾§ï¼ŒåŒ…æ‹¬äº†ä¸€äº›æ•°æ®åˆæˆçš„æ–¹æ³•ï¼Œå¼€æ”¾äº†æ›´é«˜è´¨é‡å¾—å¤šæ¨¡æ€æ•°æ®ç­‰)
6. (MixtralAI) [Pixtral 12B](https://zhuanlan.zhihu.com/p/720542712)(12Bæ¥è¿‘Qwen2-VL 72Bå’ŒLlama-3.2 90Bæ°´å¹³)
7. (Rhymes AI) [Aria: An Open Multimodal Native Mixture-of-Experts Mode](https://zhuanlan.zhihu.com/p/720542712)(ç»†ç²’åº¦æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¶æ„)
8. (Apple) [MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning](https://zhuanlan.zhihu.com/p/900354617)(appleï¼šå¤šæ¨¡æ€å¤§æ¨¡å‹ç‚¼ä¸¹æŒ‡å—)
9. (Hugging Face) [Building and better understanding vision-language models: insights and future directions](https://zhuanlan.zhihu.com/p/731680062)(Hugging Faceï¼šæ¢ç´¢å¤šæ¨¡æ€å¤§æ¨¡å‹çš„æœ€ä½³æŠ€æœ¯è·¯çº¿)
10. (Alibaba) [Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution](https://zhuanlan.zhihu.com/p/720542712)(ç²¾ç»†çš„åŠ¨æ€åˆ†è¾¨ç‡ç­–ç•¥+å¤šæ¨¡æ€æ—‹è½¬ä½ç½®åµŒå…¥)
11. [LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via Hybrid Architecture](https://huggingface.co/papers/2409.02889)(åœ¨å•ä¸ªA100 80GB GPUä¸Šå¯ä»¥å¤„ç†è¿‘åƒå¼ å›¾åƒ)
12. [MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?](https://zhuanlan.zhihu.com/p/717129017)(æœ€éš¾å¤šæ¨¡æ€Benchmark. QwenVL-2ç¬¬ä¸€ä½†æœªåŠæ ¼ï¼)
13. [VITA: Towards Open-Source Interactive Omni Multimodal LLM](https://zhuanlan.zhihu.com/p/714031459)(VITA : é¦–ä¸ªå¼€æºæ”¯æŒè‡ªç„¶äººæœºäº¤äº’çš„å…¨èƒ½å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹)
14. [Beyond LLaVA-HD: Diving into High-Resolution Large Multimodal Models](https://zhuanlan.zhihu.com/p/703258020)(é«˜æ•ˆå¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒçš„å¤šæ¨¡æ€å¤§æ¨¡å‹)
15. [Matryoshka Multimodal Models](https://zhuanlan.zhihu.com/p/700906592)(å¦‚ä½•åœ¨æ­£ç¡®å›ç­”è§†è§‰é—®é¢˜çš„åŒæ—¶ä½¿ç”¨æœ€å°‘çš„è§†è§‰æ ‡è®°ï¼Ÿ)
16. [Chameleon: Mixed-Modal Early-Fusion Foundation Models](https://zhuanlan.zhihu.com/p/698911049)(meta: æ‰€æœ‰æ¨¡æ€éƒ½å›åˆ°token regreesionä»¥è¾¾åˆ°çµæ´»çš„ç†è§£/ç”Ÿæˆ)
17. [Flamingo: a Visual Language Model for Few-Shot Learning](https://zhuanlan.zhihu.com/p/688215018)(LLMæ¯ä¸€å±‚åˆ›å»ºé¢å¤–çš„blockå¤„ç†è§†è§‰ä¿¡æ¯)
18. [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://zhuanlan.zhihu.com/p/688215018)(q-formerèåˆè§†è§‰-è¯­è¨€ä¿¡æ¯)
19. [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://zhuanlan.zhihu.com/p/688215018)(qformer+instruction tuning)
20. [Visual Instruction Tuning](https://zhuanlan.zhihu.com/p/688215018)(MLPå¯¹é½ç‰¹å¾ï¼Œgpt4vç”Ÿæˆinstruction tuningæ•°æ®)
21. [Improved Baselines with Visual Instruction Tuning](https://zhuanlan.zhihu.com/p/688215018)(å¯¹äºllavaæ•°æ®é›†ä»¥åŠæ¨¡å‹å¤§å°çš„åˆæ­¥scaling)
22. [LLaVA-NeXT: Improved reasoning, OCR, and world knowledge](https://zhuanlan.zhihu.com/p/688215018)(åˆ†è¾¨ç‡*4ï¼Œæ•°æ®é›†æ›´å¤§)
23. [Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models](https://zhuanlan.zhihu.com/p/688215018)(ä¸€ç§ç«¯åˆ°ç«¯çš„ä¼˜åŒ–æ–¹æ¡ˆï¼Œé€šè¿‡è½»é‡çº§é€‚é…å™¨è¿æ¥å›¾åƒç¼–ç å™¨å’ŒLLM)
24. [MIMIC-IT: Multi-Modal In-Context Instruction Tuning](https://zhuanlan.zhihu.com/p/688215018)( MIMIC-ITåŒ…å«å¤šä¸ªå›¾ç‰‡æˆ–è§†é¢‘çš„è¾“å…¥æ•°æ®ï¼Œå¹¶æ”¯æŒå¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¿¡æ¯)
25. [LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding](https://zhuanlan.zhihu.com/p/688215018)(ä½¿ç”¨å…¬å¼€å¯ç”¨çš„OCRå·¥å…·åœ¨LAIONæ•°æ®é›†çš„422Kä¸ªæ–‡æœ¬ä¸°å¯Œçš„å›¾åƒä¸Šæ”¶é›†ç»“æœ)
26. [SVIT: Scaling up Visual Instruction Tuning](https://zhuanlan.zhihu.com/p/688215018)(ä¸€ä¸ªåŒ…å«420ä¸‡ä¸ªè§†è§‰æŒ‡å¯¼è°ƒæ•´æ•°æ®ç‚¹çš„æ•°æ®é›†)
27. [Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond](https://zhuanlan.zhihu.com/p/688215018)(cross attentionå¯¹é½ç‰¹å¾ï¼Œæ›´å¤§çš„ç¬¬ä¸€é˜¶æ®µè®­ç»ƒæ•°æ®)
28. [NExT-GPT: Any-to-Any Multimodal LLM](https://zhuanlan.zhihu.com/p/688215018)(ç«¯åˆ°ç«¯é€šç”¨çš„ä»»æ„å¯¹ä»»æ„MM-LLMï¼ˆMultimodal-Large Language Modelï¼‰ç³»ç»Ÿ)
29. [InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition](https://zhuanlan.zhihu.com/p/688215018)(è§†è§‰ä¿¡æ¯çš„å‹ç¼©é‡‡æ ·)
30. [CogVLM: Visual Expert for Pretrained Language Models](https://zhuanlan.zhihu.com/p/688215018)(åœ¨LLMçš„å„å±‚æ·»åŠ visual expertï¼Œå®ƒå…·æœ‰ç‹¬ç«‹çš„QKVå’ŒFFNç›¸å…³çš„å‚æ•°)
31. [OtterHD: A High-Resolution Multi-modality Model](https://zhuanlan.zhihu.com/p/688215018)(ä¸“é—¨è®¾è®¡ç”¨äºä»¥ç»†ç²’åº¦ç²¾åº¦è§£é‡Šé«˜åˆ†è¾¨ç‡è§†è§‰è¾“å…¥)
32. [Monkey : Image Resolution and Text Label Are Important Things for Large Multi-modal Models](https://zhuanlan.zhihu.com/p/688215018)(Monkeyæ¨¡å‹æå‡ºäº†ä¸€ç§æœ‰æ•ˆåœ°æé«˜è¾“å…¥åˆ†è¾¨ç‡çš„æ–¹æ³•ï¼Œæœ€é«˜å¯è¾¾ 896 x 1344 åƒç´ )
33. [LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models](https://zhuanlan.zhihu.com/p/688215018)(LLaMA-VIDèµ‹äºˆç°æœ‰æ¡†æ¶æ”¯æŒé•¿è¾¾ä¸€å°æ—¶çš„è§†é¢‘ï¼Œå¹¶é€šè¿‡é¢å¤–çš„ä¸Šä¸‹æ–‡æ ‡è®°æ¨åŠ¨äº†å®ƒä»¬çš„ä¸Šé™)
34. [MoE-LLaVA: Mixture of Experts for Large Vision-Language Models](https://zhuanlan.zhihu.com/p/688215018)(è§£å†³äº†å¤šæ¨¡æ€ç¨€ç–å­¦ä¹ ä¸­çš„æ€§èƒ½ä¸‹é™é—®é¢˜)
35. [LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images](https://zhuanlan.zhihu.com/p/688215018)(é«˜æ•ˆå¤„ç†ä»»ä½•çºµæ¨ªæ¯”å’Œé«˜åˆ†è¾¨ç‡çš„å›¾åƒ)
36. [Yi-VL](https://zhuanlan.zhihu.com/p/688215018)(Yi-VLé‡‡ç”¨äº†LLaVAæ¶æ„ï¼Œç»è¿‡å…¨é¢çš„ä¸‰é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼Œä»¥å°†è§†è§‰ä¿¡æ¯ä¸Yi LLMçš„è¯­ä¹‰ç©ºé—´è‰¯å¥½å¯¹é½ï¼š)
37. [Mini-Gemini](https://zhuanlan.zhihu.com/p/693063778)(åŒè§†è§‰ç¼–ç å™¨ï¼Œä½¿ç”¨ä½åˆ†è¾¨ç‡çš„è§†è§‰ç¼–ç å™¨ç‰¹å¾ä½œä¸ºqueryï¼Œå°†é«˜åˆ†è¾¨ç‡ç‰¹å¾ä½œä¸ºkey å’Œvalueè¿›è¡Œtoken mining)
38. [Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding](https://zhuanlan.zhihu.com/p/704246896)(é‡‡ç”¨äº†ä¸€ç»„åŠ¨æ€è§†è§‰tokensæ¥ç»Ÿä¸€è¡¨ç¤ºå›¾åƒå’Œè§†é¢‘ã€‚ä½¿æ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆåˆ©ç”¨æœ‰é™æ•°é‡çš„è§†è§‰tokensï¼ŒåŒæ—¶æ•æ‰å›¾åƒæ‰€éœ€çš„ç©ºé—´ç»†èŠ‚å’Œè§†é¢‘æ‰€éœ€çš„å…¨é¢æ—¶é—´å…³ç³»ã€‚)
39. [VILA: On Pre-training for Visual Language Models](https://zhuanlan.zhihu.com/p/704246896)(äº¤é”™çš„é¢„è®­ç»ƒæ•°æ®æ˜¯æœ‰ç›Šçš„ï¼Œè€Œå•çº¯çš„å›¾åƒ-æ–‡æœ¬å¯¹å¹¶éæœ€ä½³é€‰æ‹©ã€‚)
40. [ST-LLM: Large Language Models Are Effective Temporal Learners](https://zhuanlan.zhihu.com/p/704246896)(ST-LLMæå‡ºäº†ä¸€ç§åŠ¨æ€æ©ç ç­–ç•¥ï¼Œå¹¶è®¾è®¡äº†å®šåˆ¶çš„è®­ç»ƒç›®æ ‡ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹ç‰¹åˆ«é•¿çš„è§†é¢‘ï¼Œè®¾è®¡äº†ä¸€ä¸ªå…¨å±€-å±€éƒ¨è¾“å…¥æ¨¡å—ï¼Œä»¥å¹³è¡¡æ•ˆç‡å’Œæ•ˆæœã€‚)
41. [Video-LLaVA: Learning United Visual Representation by Alignment Before Projection](https://zhuanlan.zhihu.com/p/704246896)(ç”¨è§†é¢‘ç‰¹æœ‰çš„encoderæå‡è§†é¢‘ç†è§£èƒ½åŠ›è€Œéimage encoder)

# BenchMark and Dataset
1. [MME-Reasoning: A Comprehensive Benchmark for Logical Reasoning in MLLMs](https://arxiv.org/abs/2505.21327)(ä¸‰ä¸ªç»´åº¦è¯„ä¼°å¤šæ¨¡æ€å¤§æ¨¡å‹reasoningèƒ½åŠ›)
2. [MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?](https://zhuanlan.zhihu.com/p/717129017)(æœ€éš¾å¤šæ¨¡æ€Benchmark. QwenVL-2ç¬¬ä¸€ä½†æœªåŠæ ¼ï¼)
3. [MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark](https://hub.baai.ac.cn/paper/baeaa2a4-4374-4cf3-927e-82df61ec3e8e)(MMMUçš„è¿›é˜¶ç‰ˆï¼Œæ›´æ³¨é‡å›¾åƒçš„æ„ŸçŸ¥å¯¹é—®é¢˜çš„å½±å“)
4. [From Pixels to Prose: A Large Dataset of Dense Image Captions](https://arxiv.org/pdf/2406.10328)(1600ä¸‡ç”Ÿæˆçš„image-text pairï¼Œåˆ©ç”¨å°–ç«¯çš„è§†è§‰è¯­è¨€æ¨¡å‹(Gemini 1.0 Pro Vision)è¿›è¡Œè¯¦ç»†å’Œå‡†ç¡®çš„æè¿°ã€‚)
5. [ShareGPT4Video: Improving Video Understanding and Generation with Better Captions](https://zhuanlan.zhihu.com/p/704246896)(40k from gpt4-v, 4814kç”Ÿæˆäºè‡ªå·±è®­ç»ƒçš„æ¨¡å‹)
6. [OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents](https://arxiv.org/pdf/2306.16527)(141 million web pages extracted from Common Crawl, 353 million associated images, and 115 billion text tokens)
7. [Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning](https://arxiv.org/abs/2306.14565)(åœ¨æ•°æ®å±‚é¢ï¼Œä»¥ç»†ç²’åº¦ç‰‡æ®µçº§æ›´æ­£çš„å½¢å¼æ”¶é›†äººç±»åé¦ˆï¼›åœ¨æ–¹æ³•å±‚é¢ï¼Œæˆ‘ä»¬æå‡ºäº†å¯†é›†ç›´æ¥åå¥½ä¼˜åŒ–(DDPO))
8. [Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model](https://arxiv.org/abs/2407.07053)(åœ¨æ•°æ®å±‚é¢, é€šè¿‡ä»£ç ä½œä¸ºåª’ä»‹åˆæˆæŠ½è±¡å›¾è¡¨,å¹¶ä¸” benchmarking äº†å½“å‰å¤šæ¨¡æ€æ¨¡å‹åœ¨æŠ½è±¡å›¾çš„ç†è§£ä¸Šçš„ä¸è¶³.)
# Unify Multimodal Understanding and Generation
1. [Chameleon: Mixed-Modal Early-Fusion Foundation Models](https://zhuanlan.zhihu.com/p/719475102)(Meta FAIRï¼šâ€œæ—©æœŸèåˆâ€çš„æ–¹æ³•ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè·¨æ¨¡æ€æ¨ç†å’Œç”ŸæˆçœŸæ­£çš„æ··åˆæ–‡æ¡£ã€‚)
2. [Show-o: One Single Transformer to Unify Multimodal Understanding and Generation](https://zhuanlan.zhihu.com/p/719475102)(NUS&ByteDanceï¼šæ–‡æœ¬ä½œä¸ºç¦»æ•£æ ‡è®°è¿›è¡Œè‡ªå›å½’å»ºæ¨¡ï¼Œè€Œè¿ç»­å›¾åƒåƒç´ åˆ™ä½¿ç”¨å»å™ªæ‰©æ•£å»ºæ¨¡ã€‚)
3. [Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model](https://zhuanlan.zhihu.com/p/719475102)(Metaï¼šé‡‡ç”¨äº†æ–‡æœ¬çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹å’Œå›¾åƒçš„æ‰©æ•£ä½œä¸ºç›®æ ‡å‡½æ•°,åœ¨ä¸å¢åŠ è®¡ç®—æˆæœ¬çš„å‰æä¸‹ï¼Œå®ç°äº†æ›´å¥½çš„æ¨¡æ€æ•´åˆä¸ç”Ÿæˆæ•ˆæœã€‚)
4. [VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation](https://zhuanlan.zhihu.com/p/785607305)(æ¸…å&MITï¼šç»Ÿä¸€è§†é¢‘ç†è§£ä¸ç”Ÿæˆ)
5. [MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts](https://zhuanlan.zhihu.com/p/858555788)(METAï¼šMOEæ˜¯æ··åˆæ¨¡æ€ç†è§£/ç”Ÿæˆçš„æœ€ä½³é€‰æ‹©)
6. [MIO: A Foundation Model on Multimodal Tokens](https://zhuanlan.zhihu.com/p/2186671721)(01AI: å››æ¨¡æ€ç†è§£/ç”Ÿæˆå¤§ä¸€ç»Ÿ)
7. [Harmonizing Visual Text Comprehension and Generation](https://arxiv.org/abs/2407.16364)(ECNU&ByteDanceï¼šç»“åˆè§†è§‰ç¼–ç å™¨ã€LLMã€å›¾åƒè§£ç å™¨å®ç°å¤šæ¨¡æ€è¾“å…¥è¾“å‡º)
8. [SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation](https://arxiv.org/abs/2404.14396) (Tencent AI Labï¼šé‡‡ç”¨é¢„è®­ç»ƒçš„è§†è§‰åˆ†è¯å™¨ï¼ˆå¦‚ViTï¼‰æ¥ç»Ÿä¸€å›¾åƒç†è§£å’Œç”Ÿæˆä»»åŠ¡)
9. [NExT-GPT: Any-to-Any Multimodal LLM](https://arxiv.org/abs/2309.05519)(NUSï¼šä½¿ç”¨é¢„è®­ç»ƒçš„ç¼–ç å™¨ã€æ‰©æ•£è§£ç å™¨å’ŒLLMï¼Œç»“åˆæ¨¡æ€å¯¹é½è®­ç»ƒå’ŒLoraæŒ‡ä»¤å¾®è°ƒå®ç°any2anyæ¨¡æ€ä»»åŠ¡)
10. [Any-to-Any Generation via Composable Diffusion](https://arxiv.org/abs/2305.11846)(Microsoftï¼šç»„åˆå„ç§æ¨¡æ€çš„æ‰©æ•£æ¨¡å‹ï¼Œå®ç°å¤šæ¨¡æ€å¹¶è¡Œç”Ÿæˆ)
11. [X-VILA: Cross-Modality Alignment for Large Language Model](https://arxiv.org/abs/2405.19335)(Nvidia&HKUSTï¼šå°†å•æ¨¡ç¼–ç å™¨ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å…¥å¯¹é½ï¼Œä»¥åŠå°†å•æ¨¡æ‰©æ•£è§£ç å™¨ä¸LLMçš„è¾“å‡ºå¯¹é½ï¼Œå®ç°è·¨æ¨¡æ€çš„ç†è§£ã€æ¨ç†å’Œç”Ÿæˆ)
12. [DreamLLM: Synergistic Multimodal Comprehension and Creation](https://arxiv.org/abs/2309.11499)(XJU&IIISCTï¼šè§£å†³MLLMsåœ¨å¤šæ¨¡æ€ç†è§£ä¸åˆ›é€ ä¸­çš„ååŒé—®é¢˜ï¼Œç›´æ¥åœ¨åŸå§‹å¤šæ¨¡æ€ç©ºé—´ä¸­é‡‡æ ·ï¼Œç”Ÿæˆè¯­è¨€å’Œå›¾åƒåéªŒ)
13. [Jointly Training Large Autoregressive Multimodal Models](https://arxiv.org/abs/2309.15564)(Meta AIï¼šèåˆäº†ç°æœ‰çš„æ–‡æœ¬å’Œå›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§ä¸“é—¨çš„ã€æ•°æ®é«˜æ•ˆçš„æŒ‡ä»¤è°ƒæ•´ç­–ç•¥)
14. [VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation](https://arxiv.org/abs/2312.09251)(XJU&Tencent AI Labï¼šä½¿ç”¨ä¸€ä¸ªæ–°çš„å›¾åƒåˆ†è¯å™¨-è§£ç å™¨æ¡†æ¶å°†åŸå§‹å›¾åƒè½¬æ¢ä¸ºè¿ç»­çš„è§†è§‰åµŒå…¥åºåˆ—ï¼Œä½¿ç”¨NTPè®­ç»ƒç›®æ ‡å®ç°å›¾åƒæ–‡æœ¬ç»Ÿä¸€é¢„è®­ç»ƒ)
15. [Emu:Generative pretraining in multimodality](https://arxiv.org/abs/2307.05222)(BAAI&THUï¼šä¸€ä¸ªåŸºäºTransformerçš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹é‡‡ç”¨ç»Ÿä¸€çš„è‡ªå›å½’è®­ç»ƒç›®æ ‡ï¼Œé€šè¿‡é¢„æµ‹å¤šæ¨¡æ€åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªå…ƒç´ ï¼ˆæ— è®ºæ˜¯æ–‡æœ¬æ ‡è®°è¿˜æ˜¯è§†è§‰åµŒå…¥ï¼‰è¿›è¡Œè®­ç»ƒ)
16. [Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization](https://arxiv.org/abs/2402.03161)(PKU&å¿«æ‰‹ï¼šå°†è§†é¢‘åˆ†è§£ä¸ºå…³é”®å¸§å’Œè¿åŠ¨å‘é‡ï¼Œè§†é¢‘ã€å›¾åƒå’Œæ–‡æœ¬æ•°æ®ç»Ÿä¸€ä¸º1Dç¦»æ•£æ ‡è®°)
17. [Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models](https://arxiv.org/abs/2403.18814)(CUHKï¼šä½¿ç”¨è§†è§‰åŒç¼–ç å™¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒï¼Œæ–‡æœ¬è‡ªå›å½’ç”Ÿæˆï¼Œå›¾åƒä½¿ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆ)
18. [World Model on Million-Length Video And Language With Blockwise RingAttention](https://arxiv.org/abs/2402.08268)(UC Berkeleyï¼šä½¿ç”¨VQGANå°†å›¾åƒ/è§†é¢‘ç¦»æ•£åŒ–ï¼Œç†è§£ç”Ÿæˆç»Ÿä¸€ä¸ºNTPä»»åŠ¡ï¼Œä½¿ç”¨RingAttentionã€æ¸è¿›å¼è®­ç»ƒç­‰æŠ€æœ¯å°†ä¸Šä¸‹æ–‡çª—å£æ‰©å¤§åˆ°1M tokens)
19. [Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action](https://arxiv.org/abs/2312.17172)(AI2&UIUCï¼šå°†ä¸åŒæ¨¡æ€çš„è¾“å…¥å’Œè¾“å‡ºï¼ˆå¦‚å›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘ã€åŠ¨ä½œç­‰ï¼‰æ ‡è®°åŒ–ï¼ˆtokenizeï¼‰åˆ°ä¸€ä¸ªå…±äº«çš„è¯­ä¹‰ç©ºé—´ä¸­ï¼Œç„¶åä½¿ç”¨å•ä¸€çš„ç¼–ç å™¨-è§£ç å™¨å˜æ¢å™¨æ¨¡å‹è¿›è¡Œå¤„ç†)
20. [AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling](https://arxiv.org/abs/2402.12226)(å¤æ—¦ï¼šä½¿ç”¨ç¦»æ•£çš„æ ‡è®°æ¥è¡¨ç¤ºä¸åŒçš„æ¨¡æ€ï¼ˆå¦‚å›¾åƒã€éŸ³ä¹ã€è¯­éŸ³å’Œæ–‡æœ¬ï¼‰)
21. [Write and Paint: Generative Vision-Language Models are Unified Modal Learners](https://arxiv.org/abs/2206.07699)(HKUST&ByteDanceï¼šç»“åˆå‰ç¼€è¯­è¨€å»ºæ¨¡å’Œå‰ç¼€å›¾åƒå»ºæ¨¡çš„Dacinciæ¨¡å‹)
22. [Gemini: A family of highly capable multimodal models](https://arxiv.org/abs/2312.11805)(Google Gemini Teamï¼šè§£å†³è·¨å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘å’Œæ–‡æœ¬ç†è§£çš„ä»»åŠ¡ä¸­çš„é«˜çº§æ¨ç†å’Œè¯­è¨€ç†è§£é—®é¢˜)
23. [Minigpt-5: Interleaved vision-and-language generation via generative vokens](https://arxiv.org/abs/2310.02239)(UCSCï¼šå¼•å…¥ç”Ÿæˆæ€§è§†è§‰æ ‡è®°ï¼ˆGenerative Vokensï¼‰)
24. [Mm-interleaved: Interleaved image-text generative modeling via multi-modal feature synchronizer](https://arxiv.org/abs/2401.10208)(Shanghai AI Labï¼šé›†æˆå›¾åƒç¼–ç å™¨ã€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å’Œå›¾åƒè§£ç å™¨)
25. [OMCAT: Omni Context Aware Transformer](https://arxiv.org/abs/2410.12109)(NVIDIAï¼šè·¨æ¨¡æ€æ—¶é—´ç†è§£ï¼Œåˆ©ç”¨RoTEï¼ˆRotary Time Embeddingsï¼‰é€šè¿‡åµŒå…¥ç»å¯¹å’Œç›¸å¯¹æ—¶é—´ä¿¡æ¯åˆ°éŸ³é¢‘å’Œè§†è§‰ç‰¹å¾ä¸­)
26. [Baichuan-Omni Technical Report](https://arxiv.org/abs/2410.08565)(ç™¾å·&è¥¿æ¹–å¤§å­¦&æµ™å¤§ï¼šå…¨æ¨¡æ€æ¨¡å‹)
27. [Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2410.13848)(DeepSeek-AI&HKUï¼šé’ˆå¯¹å¤šæ¨¡æ€ç†è§£å’Œå¤šæ¨¡æ€ç”Ÿæˆè§£è€¦è§†è§‰ç¼–ç )
28. [Emu3: Next-Token Prediction is All You Need](https://arxiv.org/abs/2409.18869)(BAAIï¼šè§†è§‰æ ‡è®°ç¦»æ•£åŒ–ï¼Œä½¿ç”¨DPOè¿›è¡Œå¯¹é½)
29. [VITRON: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing](https://github.com/SkyworkAI/Vitron)(NUS&NTUï¼šç¦»æ•£æ–‡æœ¬å’Œè¿ç»­ä¿¡å·çš„æ··åˆæŒ‡ä»¤ä¼ é€’æ–¹æ³•ï¼Œåƒç´ çº§æ—¶ç©ºè§†è§‰-è¯­è¨€å¯¹æ¯”å­¦ä¹ )(Neurips2024)
# Alignment With Human Preference (MLLM)
1. (CASIA) [MM-RLHF: The Next Step Forward in Multimodal LLM Alignment](https://arxiv.org/abs/2502.10391)(å…¨äººå·¥æ ‡æ³¨æ•°æ®ï¼Œæ–°ç®—æ³•ï¼Œ 27ä¸ªbenchmarké€šè¿‡dpoå…¨é¢æå‡)
2. (USTC) [DAMA: Data- and Model-aware Alignment of Multi-modal LLMs)(https://arxiv.org/abs/2502.01943)(åŠ¨æ€è°ƒæ•´betaå‚æ•°åŠ é€Ÿè§†è§‰dpoä¼˜åŒ–)
3. (Apple) [Understanding Alignment in Multimodal LLMs: A Comprehensive Study](https://zhuanlan.zhihu.com/p/6762892397)(é€šè¿‡ç‹¬ç«‹åˆ†æå„ä¸ªå› ç´ ï¼Œæ¢ç´¢ä¸åŒçš„å¯¹é½æ–¹æ³•å¯¹MLLMsæ€§èƒ½çš„å½±å“)
4. [Aligning Large Multimodal Models with Factually Augmented RLHF](https://llava-rlhf.github.io/)
5. [CLIP-DPO: Vision-Language Models as a Source of Preference for Fixing Hallucinations in LVLMs](https://www.arxiv.org/abs/2408.10433)(ä½¿ç”¨é¢„è®­ç»ƒçš„ CLIP æ¨¡å‹å¯¹ LVLM è‡ªç”Ÿæˆçš„æ ‡é¢˜è¿›è¡Œæ’åºï¼Œä»¥æ„å»º DPO çš„æ­£è´Ÿå¯¹)
6. [ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models](https://www.arxiv.org/abs/2409.09318)(é€‰æ‹©äº†ä¸€ç§åŠ¨æ€ç”Ÿæˆæ–¹æ³•æ¥åˆ›å»ºä¸€ä¸ª open-set benchmarkï¼Œå¼•å…¥äº†å¼€æ”¾é›†åŠ¨æ€è¯„ä¼°åè®®(ODE)ï¼Œä¸“é—¨ç”¨äºè¯„ä¼° MLLM ä¸­çš„å¯¹è±¡å­˜åœ¨å¹»è§‰)
7. [Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization](https://arxiv.org/abs/2311.16839)(æœ¬æ–‡å°†æ¶ˆé™¤å¹»è§‰è§†ä¸ºä¸€ç§æ¨¡å‹åå¥½ï¼Œä½¿æ¨¡å‹åå‘äºæ— å¹»è§‰è¾“å‡ºï¼Œäºæ˜¯æå‡ºäº†ä¸€ç§å¯¹å¹»è§‰æ•æ„Ÿçš„å¤šæ¨¡æ€DPO ç­–ç•¥ â€”â€” HA-DPOã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†å¥å­çº§å¹»è§‰æ¯”ç‡(SHR)ï¼Œå®ƒä¸å—å›ºå®šç±»åˆ«å’ŒèŒƒå›´çš„é™åˆ¶ï¼Œä¸ºå¤šæ¨¡æ€å¹»è§‰æä¾›äº†å¹¿æ³›ã€ç»†ç²’åº¦å’Œå®šé‡çš„æµ‹é‡)
8. [Detecting and Preventing Hallucinations in Large Vision Language Models](https://arxiv.org/abs/2308.06394)(ä¸ºäº†ä¾¿äºè‡ªåŠ¨æ£€æµ‹å¹»è§‰ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨ InstructBLIP çš„ VQA å“åº”æ„å»ºäº†ä¸€ä¸ªå¤šæ ·åŒ–çš„äººå·¥æ ‡è®°æ•°æ®é›† M-HalDetectï¼Œä¸“æ³¨äºåœ¨è¯¦ç»†å›¾åƒæè¿°çš„å­å¥çº§åˆ«ä¸Šè¿›è¡Œç»†ç²’åº¦æ³¨é‡Šã€‚åœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒä¸åŒå¯†åº¦(å¥å­çº§ï¼Œå­å¥å­çº§)çš„å¤šä¸ªå¥–åŠ±æ¨¡å‹ï¼Œç”¨äºå¹»è§‰æ£€æµ‹ã€‚æˆ‘ä»¬ä¹Ÿä½¿ç”¨ç»†ç²’åº¦ç›´æ¥åå¥½ä¼˜åŒ–(FDPO)ç›´æ¥ä¼˜åŒ– InstructBLIP)
9. [RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness](https://arxiv.org/abs/2405.17220)(åŒä¸€ä¸ªå¤§æ¨¡å‹ç”Ÿæˆå¤šä¸ªå›å¤ï¼Œå°†å›å¤æŒ‰å¥æ‹†åˆ†ï¼Œä¹‹åè½¬åŒ–ä¸ºé—®å¥è®©å¼€æºæ¨¡å‹å›å¤å‡†ç¡®åº¦ï¼Œå°†æ‰€æœ‰å‡†ç¡®åº¦ç›¸åŠ ï¼Œå¾—åˆ°åå¥½æ•°æ®ï¼Œç”¨äºè¿­ä»£DPO)
10. [Enhancing Visual-Language Modality Alignment in Large Vision Language Models via Self-Improvement](https://arxiv.org/abs/2405.15973)(æˆ‘ä»¬æå‡ºäº† Self-Improvement Modality Alignment(SIMA)ï¼Œæ—¨åœ¨é€šè¿‡è‡ªæˆ‘å®Œå–„æœºåˆ¶è¿›ä¸€æ­¥æ”¹å–„ LVLM å†…è§†è§‰æ¨¡æ€å’Œè¯­è¨€æ¨¡æ€ä¹‹é—´çš„å¯¹é½)
11. [MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models](https://arxiv.org/abs/2410.17637)(å°†æ— å…³çš„å•å›¾åƒæ•°æ®æ‹¼æ¥ä¸ºåºåˆ—ã€ç½‘æ ¼ã€ç”»ä¸­ç”»æ•°æ®ï¼Œé€šè¿‡æ³¨æ„åŠ›å€¼åœ¨æ­£ç¡®ç›®æ ‡ä¸Šçš„å¤šå°‘æ¥é€‰æ‹©åå¥½æ•°æ®ï¼Œç»è¿‡è¿‡æ»¤å¾—åˆ°æ•°æ®ï¼Œç”¨äºDPO)
12. [CHiP: Cross-modal Hierarchical Direct Preference Optimization for Multimodal LLMs](https://openreview.net/forum?id=7lpDn2MhM2)(ä¸ºäº†ä½¿è§†è§‰ä¿¡æ¯å¯¹é½ï¼Œå¼•å…¥äº†åˆ†å±‚æ–‡æœ¬åå¥½ä¼˜åŒ–æ¨¡å—ï¼Œåˆ†åˆ«ä¸ºå›å¤çº§ã€ç‰‡æ®µçº§ã€tokençº§åå¥½ä¼˜åŒ–ï¼›åŒæ—¶å¼•å…¥äº†è§†è§‰åå¥½ä¼˜åŒ–)
13. [3D-CT-GPT++: Enhancing 3D Radiology Report Generation with Direct Preference Optimization and Large Vision-Language Models](https://openreview.net/forum?id=LzycEbgLoi)(å°†æ— å…³çš„å•å›¾åƒæ•°æ®æ‹¼æ¥ä¸ºåºåˆ—ã€ç½‘æ ¼ã€ç”»ä¸­ç”»æ•°æ®ï¼Œé€šè¿‡æ³¨æ„åŠ›å€¼åœ¨æ­£ç¡®ç›®æ ‡ä¸Šçš„å¤šå°‘æ¥é€‰æ‹©åå¥½æ•°æ®ï¼Œç»è¿‡è¿‡æ»¤å¾—åˆ°æ•°æ®ï¼Œç”¨äºDPO)
14. [MAVIS: Mathematical Visual Instruction Tuning with an Automatic Data Engine](https://openreview.net/forum?id=MnJzJ2gvuf)(é¦–å…ˆé€šè¿‡å¯¹æ¯”å­¦ä¹ æ¥å¾®è°ƒæ•°å­¦ç‰¹å®šçš„è§†è§‰ç¼–ç å™¨ï¼Œéšåå°†è¯¥ç¼–ç å™¨ä¸LLMå¯¹é½ï¼Œä¹‹åï¼Œé‡‡ç”¨MAVIS-Instructè¿›è¡ŒæŒ‡ä»¤è°ƒæ•´ï¼Œæœ€åï¼Œåœ¨MAVIS-Instructä¸­ä½¿ç”¨å¸¦æœ‰æ³¨é‡Šçš„CoTåŸºæœ¬åŸç†çš„DPO)
15. [HomieBot: an Adaptive System for Embodied Mobile Manipulation in Open Environments](https://openreview.net/forum?id=NQTrARs2pz)(ç”±100ä¸ªå¤æ‚çš„æ—¥å¸¸ä»»åŠ¡ç»„æˆï¼Œä»Replica Challengeä¸­æŠ½å–äº†100ä¸ªä¸åŒçš„ç‰‡æ®µæ¥æ„å»ºåœºæ™¯å¹¶è®¾è®¡ä»»åŠ¡ï¼Œåªä½¿ç”¨Replica Challengeçš„é…ç½®æ–‡ä»¶æ¥æ„é€ åœºæ™¯ã€‚æ‰‹åŠ¨æ§åˆ¶æœºå™¨äººå®Œæˆæ‰€æœ‰ä»»åŠ¡ï¼Œå°†æ‰§è¡Œè¿‡ç¨‹åˆ†è§£ä¸ºå‡ ä¸ªå­ä»»åŠ¡ï¼Œæœ€ç»ˆå¾—åˆ°966ä¸ªå­ä»»åŠ¡ã€‚ä½¿ç”¨GPT-4å°†æœ€ç»ˆä»»åŠ¡çš„æ–‡æœ¬æè¿°å’Œæ¯ä¸ªå­ä»»åŠ¡çš„åˆ†æé‡æ–°ç”Ÿæˆä¸‰æ¬¡ï¼Œå°†å®ƒä»¬é‡å†™ä¸ºå…·æœ‰ç›¸åŒå«ä¹‰ä½†ä¸åŒè¡¨è¾¾çš„æ–‡æœ¬ï¼Œå¾—åˆ°3720ä¸ªSFTæ•°æ®ã€‚é€šè¿‡æ›¿æ¢éƒ¨åˆ†å†…å®¹å¾—åˆ°10104ä¸ªDPOæ•°æ®)
16. [InteractiveCOT: Aligning Dynamic Chain-of-Thought Planning for Embodied Decision-Making](https://openreview.net/forum?id=Y4iaDU4yMi)(é¦–å…ˆä½¿ç”¨å¼€æºæ•°æ®é›†LEVI-Project/sft-dataå¯¹llava-v1.6-mistral-7bè¿›è¡Œsftå¾®è°ƒï¼Œç„¶åä½¿ç”¨æ¨¡å‹ä¸ç¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œåœ¨è¿™äº›äº¤äº’è¿‡ç¨‹ä¸­ä¼˜åŒ–å…¶CoTèƒ½åŠ›ï¼Œå¹¶åœ¨è®­ç»ƒæœŸé—´å®æ—¶ç›‘æ§æ€§èƒ½)
17. [vVLM: Exploring Visual Reasoning in VLMs against Language Priors](https://openreview.net/forum?id=lCqNxBGPp5)(é€šè¿‡æ‰°åŠ¨æ¥ç ´åå›¾åƒï¼ŒåŒæ—¶ä¿æŒæ–‡æœ¬(é—®é¢˜å’Œç­”æ¡ˆ)ä¸å˜ï¼Œä»è€Œæ„å»ºè¢«é€‰ä¸­å’Œè¢«æ‹’ç»çš„åå¥½å¯¹ã€‚åº”ç”¨äºå›¾åƒçš„æ‰°åŠ¨åŒ…æ‹¬è¯­ä¹‰ç¼–è¾‘ã€é«˜æ–¯æ¨¡ç³Šå’Œåƒç´ åŒ–)
18. [AdPO: Enhancing the Adversarial Robustness of Large Vision-Language Models with Preference Optimization](https://openreview.net/forum?id=nbngu7H3ko)(é€šè¿‡PGDç­‰è¿­ä»£ä¼˜åŒ–è·å¾—å¯¹æŠ—å›¾åƒï¼ˆå¯¹æŠ—æ€§å›¾åƒæ˜¯é€šè¿‡åœ¨åŸå§‹å›¾åƒä¸­å¼•å…¥å¾®å°çš„ã€å‡ ä¹éš¾ä»¥å¯Ÿè§‰çš„æ‰°åŠ¨æ¥ç”Ÿæˆçš„ï¼‰ï¼Œç”¨åŸå§‹å›¾åƒä¸å¯¹æŠ—å›¾åƒç”Ÿæˆå¯¹åº”çš„æè¿°æ–‡æœ¬ä½œä¸ºåå¥½æ•°æ®è¿›è¡ŒDPOï¼ŒåŒæ—¶å¼•å…¥äº†å¯¹æŠ—æ€§å›¾åƒä¼˜åŒ–)
19. [Enhancing Multimodal LLM for Detailed and Accurate Video Captioning using Multi-Round Preference Optimization](https://openreview.net/forum?id=ufi0WPTgWp)(é¦–å…ˆåœ¨å¤§å‹éŸ³é¢‘æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒéŸ³é¢‘å¯¹é½å™¨å®ç°éŸ³é¢‘æ¨¡æ€å¯¹é½ï¼Œç„¶åè¿›è¡Œaudio-visual SFTï¼Œä¹‹ååº”ç”¨åŸºäºmrDPOçš„RLï¼Œæœ€åé‡ç”Ÿå¾®è°ƒ)
20. [Aligning Visual Contrastive learning models via Preference Optimization](https://openreview.net/forum?id=wgRQ2WAORJ)(Step 1: Response generation. Step 2: Scoring. Step 3: Reward Preference. Iterative Improvement.)
21. [SQuBa: Speech Mamba Language Model with Querying-Attention for Efficient Summarization](https://openreview.net/forum?id=zOMa82W1HV)(ä¸¤é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ã€‚åœ¨å¯¹å‡†é˜¶æ®µï¼Œåªæœ‰projectorä½¿ç”¨ASRä»»åŠ¡è¿›è¡Œè®­ç»ƒã€‚åœ¨å¾®è°ƒé˜¶æ®µï¼ŒLLM backbone and the projectoréƒ½æ¥å—summarizationä»»åŠ¡çš„è®­ç»ƒã€‚å¾®è°ƒç»“æŸåè¿›è¡Œç¦»çº¿è‡ªç”ŸæˆDPOã€‚)

# Alignment With Human Preference (LLM)

1. [ChatGLM-Mathï¼šImproving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline](https://zhuanlan.zhihu.com/p/698983475)(ChatGLM-Math: Self-Critiqueè¿­ä»£å¯¹é½æ˜¾è‘—æå‡æ•°å­¦èƒ½åŠ›)
2. [Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct Preference Optimization](https://zhuanlan.zhihu.com/p/698782623)(å¤§è¯­è¨€æ¨¡å‹çš„å¤šç›®æ ‡å¯¹é½)
3. [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://zhuanlan.zhihu.com/p/693163438)(ç›´æ¥åå¥½ä¼˜åŒ–å…‹æœRLHFä¸ç¨³å®šçš„é—®é¢˜)
4. [KTO: Model Alignment as Prospect Theoretic Optimization](https://zhuanlan.zhihu.com/p/693163438)(ä¸éœ€è¦æˆå¯¹æ•°æ®çš„åå¥½ä¼˜åŒ–)
5. [Direct Preference Optimization with an Offset](https://zhuanlan.zhihu.com/p/693163438)(å¸¦åç§»çš„DPO, è¦æ±‚é¦–é€‰å“åº”å’Œä¸å—æ¬¢è¿å“åº”ä¹‹é—´çš„å¯èƒ½æ€§å·®å¼‚å¤§äºä¸€ä¸ªåç§»å€¼)
6. [Contrastive preference learning: Learning from human feedback without reinforcement learning](https://zhuanlan.zhihu.com/p/693163438)(å¯¹æ¯”åå¥½å­¦ä¹ ï¼ˆCPLï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•ç”¨äºä»åå¥½ä¸­å­¦ä¹ æœ€ä¼˜ç­–ç•¥è€Œæ— éœ€å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œä»è€Œé¿å…äº†å¯¹RLçš„éœ€æ±‚)
7. [Statistical Rejection Sampling Improves Preference Optimization](https://zhuanlan.zhihu.com/p/693163438)(ä½¿ç”¨æ‹’ç»æŠ½æ ·ä»ç›®æ ‡æœ€ä¼˜ç­–ç•¥ä¸­è·å–åå¥½æ•°æ®ï¼Œä»è€Œæ›´å‡†ç¡®åœ°ä¼°è®¡æœ€ä¼˜ç­–ç•¥)
8. [Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study](https://zhuanlan.zhihu.com/p/693163438)(åœ¨æ‰€æœ‰å®éªŒä¸­ï¼ŒPPOå§‹ç»ˆä¼˜äºDPOã€‚ç‰¹åˆ«æ˜¯åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„ä»£ç ç«èµ›ä»»åŠ¡ä¸­ï¼ŒPPOå®ç°äº†æœ€å…ˆè¿›çš„ç»“æœ)
9. [Fine-tuning Aligned Language Models Compromises Safety](https://zhuanlan.zhihu.com/p/696707347)(å¾®è°ƒå¯¹é½çš„è¯­è¨€æ¨¡å‹ä¼šæŸå®³å®‰å…¨æ€§)
10. [ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline](https://mp.weixin.qq.com/s/lg7ueR9b-om0ecUEoT4x8w)(reward model, Rejective Fine-tuning, then DPOè¿­ä»£æå‡æ¨¡å‹æ•°å­¦æ€§èƒ½)
11. [SimPO: Simple Preference Optimization with a Reference-Free Reward](https://zhuanlan.zhihu.com/p/700438956)(length reg+å»æ‰ref model)
12. [towards analyzing and understanding the limitations of dpo: a theoretical perspective](https://zhuanlan.zhihu.com/p/701213691)(DPOçš„å®é™…ä¼˜åŒ–è¿‡ç¨‹å¯¹SFTåçš„LLMså¯¹é½èƒ½åŠ›çš„åˆå§‹æ¡ä»¶ä¸ºä»€ä¹ˆæ•æ„Ÿ)
13. [Iterative Length-Regularized Direct Preference Optimization: A Case Study on Improving 7B Language Models to GPT-4 Level](https://arxiv.org/abs/2406.11817)(è¡¨æ˜è¿­ä»£ DPO (iDPO)å¯ä»¥é€šè¿‡ç²¾å¿ƒè®¾è®¡å°† 7B æ¨¡å‹çš„ LC win rate å¢å¼ºåˆ° GPT-4 æ°´å¹³)
14. [Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs](https://arxiv.org/abs/2406.18629)(å‡ºäº†ä¸€ç§æœ‰æ•ˆä¸”ç»æµçš„ pipeline æ¥æ”¶é›†æˆå¯¹æ•°å­¦é—®é¢˜åå¥½æ•°æ®ã€‚å¼•å…¥äº† Step-DPOï¼Œæœ€å¤§åŒ–ä¸‹ä¸€ä¸ªæ¨ç†æ­¥éª¤æ­£ç¡®çš„æ¦‚ç‡ï¼Œæœ€å°åŒ–å…¶é”™è¯¯çš„æ¦‚ç‡)
15. [A Novel Soft Alignment Approach for Language Models with Explicit Listwise Rewards](https://openreview.net/forum?id=28TLorTMnP)(é€šè¿‡åœ¨ç°æœ‰å¼ºå¤§çš„LLMçš„æŒ‡å¯¼ä¸‹å¯¹æ¯”å¤šä¸ªæ•°æ®ç‚¹ï¼Œå°†ç”Ÿæˆå»ºæ¨¡é—®é¢˜è½¬åŒ–ä¸ºåˆ†ç±»ä»»åŠ¡ã€‚SPOæŸå¤±å¯ä»¥çœ‹ä½œæ˜¯kç±»äº¤å‰ç†µæŸå¤±ï¼Œå¸¦æœ‰æ›´å¼ºå¤§çš„æ•™å¸ˆLLMæä¾›çš„è½¯æ ‡ç­¾)
16. [Montessori-Instruct: Generate Influential Training Data Tailored for Student Learning](https://arxiv.org/abs/2410.14208)(æ•™å¸ˆæ¨¡å‹æ ¹æ®ä½¿ç”¨Self-Instructç”Ÿæˆæ•°æ®é›†ï¼Œç„¶åæ”¶é›†è¿™äº›æ•°æ®ç‚¹çš„æœ¬åœ°æ•°æ®å¯¹å­¦ç”Ÿæ¨¡å‹çš„å½±å“ï¼Œæ”¶é›†åˆ°çš„æ•°æ®åå¥½å½¢æˆåå¥½æ•°æ®é›†ï¼Œç„¶åç”¨DPOæ›´æ–°æ•™å¸ˆæ¨¡å‹ï¼Œè¯¥è¿‡ç¨‹å¯ä»¥è¿­ä»£å¤šè½®ï¼Œä»¥æ ¹æ®å­¦ç”Ÿæ›´æ–°çš„åå¥½ä¸æ–­æ”¹è¿›æ•™å¸ˆ)
17. [Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts](https://openreview.net/forum?id=APDnmucgID)(ä½œè€…è®¤ä¸ºç›¸ä¼¼çš„é—®é¢˜ç”Ÿæˆçš„ç­”æ¡ˆåº”è¯¥ä¹Ÿå¯ä»¥ç”¨æ¥åå¥½å­¦ä¹ ï¼Œäºæ˜¯å€ŸåŠ©å¯¹æ¯”çŸ©é˜µæ¥ç ”ç©¶æ­¤é—®é¢˜,æå‡ºäº†3ç§å¯é€‚ç”¨çš„ç®—æ³•)
