# Awesome-Multimodal-Large-Language-Models
his is a repository for organizing articles related to Multimodal Large Language Models, Large Language Models, and Diffusion Models; Most papers are linked to **my reading notes**. Feel free to visit my [personal homepage](https://yfzhang114.github.io/) and contact me for collaboration and discussion.


### About Me :high_brightness: 
I'm a third-year Ph.D. student at the State Key Laboratory of Pattern Recognition, the University of Chinese Academy of Sciences, advised by Prof. [Tieniu Tan](http://people.ucas.ac.cn/~tantieniu). I have also spent time at Microsoft, advised by Prof. [Jingdong Wang](https://jingdongwang2017.github.io/), alibaba DAMO Academy, work with Prof. [Rong Jin](https://scholar.google.com/citations?user=CS5uNscAAAAJ&hl=zh-CN).


###  ğŸ”¥ Updated 2024-06-21
- Recent Modality Bridging papers of Multimodal Large Language Models have been updated.
- Our paper  [Debiasing Multimodal Large Language Models](https://arxiv.org/abs/2403.05262) has been released.  [[Code]](https://github.com/yfzhang114/LLaVA-Align) [[Reading Notes]](https://zhuanlan.zhihu.com/p/686461442)

# Table of Contents (ongoing)
* [Multimodal Large Language Models](#multimodal-large-language-models)
* [BenchMark and Dataset](#benchmark-and-dataset)
* [Diffusion Models](#diffusion-models)
* [Hallucination](#hallucination)
* [Alignment With Human Preference](#alignment-with-human-preference)
# Multimodal Large Language Models
1. [Beyond LLaVA-HD: Diving into High-Resolution Large Multimodal Models](https://github.com/yfzhang114/SliME)(é«˜æ•ˆå¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒçš„å¤šæ¨¡æ€å¤§æ¨¡å‹)
2. [Matryoshka Multimodal Models](https://zhuanlan.zhihu.com/p/700906592)(å¦‚ä½•åœ¨æ­£ç¡®å›ç­”è§†è§‰é—®é¢˜çš„åŒæ—¶ä½¿ç”¨æœ€å°‘çš„è§†è§‰æ ‡è®°ï¼Ÿ)
3. [Chameleon: Mixed-Modal Early-Fusion Foundation Models](https://zhuanlan.zhihu.com/p/698911049)(meta: æ‰€æœ‰æ¨¡æ€éƒ½å›åˆ°token regreesionä»¥è¾¾åˆ°çµæ´»çš„ç†è§£/ç”Ÿæˆ)
4. [Flamingo: a Visual Language Model for Few-Shot Learning](https://zhuanlan.zhihu.com/p/688215018)(LLMæ¯ä¸€å±‚åˆ›å»ºé¢å¤–çš„blockå¤„ç†è§†è§‰ä¿¡æ¯)
5. [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://zhuanlan.zhihu.com/p/688215018)(q-formerèåˆè§†è§‰-è¯­è¨€ä¿¡æ¯)
6. [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://zhuanlan.zhihu.com/p/688215018)(qformer+instruction tuning)
7. [Visual Instruction Tuning](https://zhuanlan.zhihu.com/p/688215018)(MLPå¯¹é½ç‰¹å¾ï¼Œgpt4vç”Ÿæˆinstruction tuningæ•°æ®)
8. [Improved Baselines with Visual Instruction Tuning](https://zhuanlan.zhihu.com/p/688215018)(å¯¹äºllavaæ•°æ®é›†ä»¥åŠæ¨¡å‹å¤§å°çš„åˆæ­¥scaling)
9. [LLaVA-NeXT: Improved reasoning, OCR, and world knowledge](https://zhuanlan.zhihu.com/p/688215018)(åˆ†è¾¨ç‡*4ï¼Œæ•°æ®é›†æ›´å¤§)
10. [Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models](https://zhuanlan.zhihu.com/p/688215018)(ä¸€ç§ç«¯åˆ°ç«¯çš„ä¼˜åŒ–æ–¹æ¡ˆï¼Œé€šè¿‡è½»é‡çº§é€‚é…å™¨è¿æ¥å›¾åƒç¼–ç å™¨å’ŒLLM)
11. [MIMIC-IT: Multi-Modal In-Context Instruction Tuning](https://zhuanlan.zhihu.com/p/688215018)( MIMIC-ITåŒ…å«å¤šä¸ªå›¾ç‰‡æˆ–è§†é¢‘çš„è¾“å…¥æ•°æ®ï¼Œå¹¶æ”¯æŒå¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¿¡æ¯)
12. [LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding](https://zhuanlan.zhihu.com/p/688215018)(ä½¿ç”¨å…¬å¼€å¯ç”¨çš„OCRå·¥å…·åœ¨LAIONæ•°æ®é›†çš„422Kä¸ªæ–‡æœ¬ä¸°å¯Œçš„å›¾åƒä¸Šæ”¶é›†ç»“æœ)
13. [SVIT: Scaling up Visual Instruction Tuning](https://zhuanlan.zhihu.com/p/688215018)(ä¸€ä¸ªåŒ…å«420ä¸‡ä¸ªè§†è§‰æŒ‡å¯¼è°ƒæ•´æ•°æ®ç‚¹çš„æ•°æ®é›†)
14. [Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond](https://zhuanlan.zhihu.com/p/688215018)(cross attentionå¯¹é½ç‰¹å¾ï¼Œæ›´å¤§çš„ç¬¬ä¸€é˜¶æ®µè®­ç»ƒæ•°æ®)
15. [NExT-GPT: Any-to-Any Multimodal LLM](https://zhuanlan.zhihu.com/p/688215018)(ç«¯åˆ°ç«¯é€šç”¨çš„ä»»æ„å¯¹ä»»æ„MM-LLMï¼ˆMultimodal-Large Language Modelï¼‰ç³»ç»Ÿ)
16. [InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition](https://zhuanlan.zhihu.com/p/688215018)(è§†è§‰ä¿¡æ¯çš„å‹ç¼©é‡‡æ ·)
17. [CogVLM: Visual Expert for Pretrained Language Models](https://zhuanlan.zhihu.com/p/688215018)(åœ¨LLMçš„å„å±‚æ·»åŠ visual expertï¼Œå®ƒå…·æœ‰ç‹¬ç«‹çš„QKVå’ŒFFNç›¸å…³çš„å‚æ•°)
18. [OtterHD: A High-Resolution Multi-modality Model](https://zhuanlan.zhihu.com/p/688215018)(ä¸“é—¨è®¾è®¡ç”¨äºä»¥ç»†ç²’åº¦ç²¾åº¦è§£é‡Šé«˜åˆ†è¾¨ç‡è§†è§‰è¾“å…¥)
19. [Monkey : Image Resolution and Text Label Are Important Things for Large Multi-modal Models](https://zhuanlan.zhihu.com/p/688215018)(Monkeyæ¨¡å‹æå‡ºäº†ä¸€ç§æœ‰æ•ˆåœ°æé«˜è¾“å…¥åˆ†è¾¨ç‡çš„æ–¹æ³•ï¼Œæœ€é«˜å¯è¾¾ 896 x 1344 åƒç´ )
20. [LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models](https://zhuanlan.zhihu.com/p/688215018)(LLaMA-VIDèµ‹äºˆç°æœ‰æ¡†æ¶æ”¯æŒé•¿è¾¾ä¸€å°æ—¶çš„è§†é¢‘ï¼Œå¹¶é€šè¿‡é¢å¤–çš„ä¸Šä¸‹æ–‡æ ‡è®°æ¨åŠ¨äº†å®ƒä»¬çš„ä¸Šé™)
21. [MoE-LLaVA: Mixture of Experts for Large Vision-Language Models](https://zhuanlan.zhihu.com/p/688215018)(è§£å†³äº†å¤šæ¨¡æ€ç¨€ç–å­¦ä¹ ä¸­çš„æ€§èƒ½ä¸‹é™é—®é¢˜)
22. [LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images](https://zhuanlan.zhihu.com/p/688215018)(é«˜æ•ˆå¤„ç†ä»»ä½•çºµæ¨ªæ¯”å’Œé«˜åˆ†è¾¨ç‡çš„å›¾åƒ)
23. [Yi-VL](https://zhuanlan.zhihu.com/p/688215018)(Yi-VLé‡‡ç”¨äº†LLaVAæ¶æ„ï¼Œç»è¿‡å…¨é¢çš„ä¸‰é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼Œä»¥å°†è§†è§‰ä¿¡æ¯ä¸Yi LLMçš„è¯­ä¹‰ç©ºé—´è‰¯å¥½å¯¹é½ï¼š)
24. [Mini-Gemini](https://zhuanlan.zhihu.com/p/693063778)(åŒè§†è§‰ç¼–ç å™¨ï¼Œä½¿ç”¨ä½åˆ†è¾¨ç‡çš„è§†è§‰ç¼–ç å™¨ç‰¹å¾ä½œä¸ºqueryï¼Œå°†é«˜åˆ†è¾¨ç‡ç‰¹å¾ä½œä¸ºkey å’Œvalueè¿›è¡Œtoken mining)
25. [Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding](https://zhuanlan.zhihu.com/p/704246896)(é‡‡ç”¨äº†ä¸€ç»„åŠ¨æ€è§†è§‰tokensæ¥ç»Ÿä¸€è¡¨ç¤ºå›¾åƒå’Œè§†é¢‘ã€‚ä½¿æ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆåˆ©ç”¨æœ‰é™æ•°é‡çš„è§†è§‰tokensï¼ŒåŒæ—¶æ•æ‰å›¾åƒæ‰€éœ€çš„ç©ºé—´ç»†èŠ‚å’Œè§†é¢‘æ‰€éœ€çš„å…¨é¢æ—¶é—´å…³ç³»ã€‚)
26. [VILA: On Pre-training for Visual Language Models](https://zhuanlan.zhihu.com/p/704246896)(äº¤é”™çš„é¢„è®­ç»ƒæ•°æ®æ˜¯æœ‰ç›Šçš„ï¼Œè€Œå•çº¯çš„å›¾åƒ-æ–‡æœ¬å¯¹å¹¶éæœ€ä½³é€‰æ‹©ã€‚)
27. [ST-LLM: Large Language Models Are Effective Temporal Learners](https://zhuanlan.zhihu.com/p/704246896)(ST-LLMæå‡ºäº†ä¸€ç§åŠ¨æ€æ©ç ç­–ç•¥ï¼Œå¹¶è®¾è®¡äº†å®šåˆ¶çš„è®­ç»ƒç›®æ ‡ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹ç‰¹åˆ«é•¿çš„è§†é¢‘ï¼Œè®¾è®¡äº†ä¸€ä¸ªå…¨å±€-å±€éƒ¨è¾“å…¥æ¨¡å—ï¼Œä»¥å¹³è¡¡æ•ˆç‡å’Œæ•ˆæœã€‚)
28. [Video-LLaVA: Learning United Visual Representation by Alignment Before Projection](https://zhuanlan.zhihu.com/p/704246896)(ç”¨è§†é¢‘ç‰¹æœ‰çš„encoderæå‡è§†é¢‘ç†è§£èƒ½åŠ›è€Œéimage encoder)

# BenchMark and Dataset
1. [From Pixels to Prose: A Large Dataset of Dense Image Captions](https://arxiv.org/pdf/2406.10328)(1600ä¸‡ç”Ÿæˆçš„image-text pairï¼Œåˆ©ç”¨å°–ç«¯çš„è§†è§‰è¯­è¨€æ¨¡å‹(Gemini 1.0 Pro Vision)è¿›è¡Œè¯¦ç»†å’Œå‡†ç¡®çš„æè¿°ã€‚)
2. [ShareGPT4Video: Improving Video Understanding and Generation with Better Captions](https://zhuanlan.zhihu.com/p/704246896)(40k from gpt4-v, 4814kç”Ÿæˆäºè‡ªå·±è®­ç»ƒçš„æ¨¡å‹)
3. [OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents](https://arxiv.org/pdf/2306.16527)(141 million web pages extracted from Common Crawl, 353 million associated images, and 115 billion text tokens)
# Diffusion Models

# Hallucination

# Alignment With Human Preference

1. [Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct Preference Optimization](https://zhuanlan.zhihu.com/p/698782623)(å¤§è¯­è¨€æ¨¡å‹çš„å¤šç›®æ ‡å¯¹é½)
2. [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://zhuanlan.zhihu.com/p/693163438)(ç›´æ¥åå¥½ä¼˜åŒ–å…‹æœRLHFä¸ç¨³å®šçš„é—®é¢˜)
3. [KTO: Model Alignment as Prospect Theoretic Optimization](https://zhuanlan.zhihu.com/p/693163438)(ä¸éœ€è¦æˆå¯¹æ•°æ®çš„åå¥½ä¼˜åŒ–)
4. [Direct Preference Optimization with an Offset](https://zhuanlan.zhihu.com/p/693163438)(å¸¦åç§»çš„DPO, è¦æ±‚é¦–é€‰å“åº”å’Œä¸å—æ¬¢è¿å“åº”ä¹‹é—´çš„å¯èƒ½æ€§å·®å¼‚å¤§äºä¸€ä¸ªåç§»å€¼)
5. [Contrastive preference learning: Learning from human feedback without reinforcement learning](https://zhuanlan.zhihu.com/p/693163438)(å¯¹æ¯”åå¥½å­¦ä¹ ï¼ˆCPLï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•ç”¨äºä»åå¥½ä¸­å­¦ä¹ æœ€ä¼˜ç­–ç•¥è€Œæ— éœ€å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œä»è€Œé¿å…äº†å¯¹RLçš„éœ€æ±‚)
6. [Statistical Rejection Sampling Improves Preference Optimization](https://zhuanlan.zhihu.com/p/693163438)(ä½¿ç”¨æ‹’ç»æŠ½æ ·ä»ç›®æ ‡æœ€ä¼˜ç­–ç•¥ä¸­è·å–åå¥½æ•°æ®ï¼Œä»è€Œæ›´å‡†ç¡®åœ°ä¼°è®¡æœ€ä¼˜ç­–ç•¥)
7. [Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study](https://zhuanlan.zhihu.com/p/693163438)(åœ¨æ‰€æœ‰å®éªŒä¸­ï¼ŒPPOå§‹ç»ˆä¼˜äºDPOã€‚ç‰¹åˆ«æ˜¯åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„ä»£ç ç«èµ›ä»»åŠ¡ä¸­ï¼ŒPPOå®ç°äº†æœ€å…ˆè¿›çš„ç»“æœ)
8. [Fine-tuning Aligned Language Models Compromises Safety](https://zhuanlan.zhihu.com/p/696707347)(å¾®è°ƒå¯¹é½çš„è¯­è¨€æ¨¡å‹ä¼šæŸå®³å®‰å…¨æ€§)
9. [ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline](https://mp.weixin.qq.com/s/lg7ueR9b-om0ecUEoT4x8w)(reward model, Rejective Fine-tuning, then DPOè¿­ä»£æå‡æ¨¡å‹æ•°å­¦æ€§èƒ½)
10. [SimPO: Simple Preference Optimization with a Reference-Free Reward](https://zhuanlan.zhihu.com/p/700438956)(length reg+å»æ‰ref model)
11. [towards analyzing and understanding the limitations of dpo: a theoretical perspective](https://zhuanlan.zhihu.com/p/701213691)(DPOçš„å®é™…ä¼˜åŒ–è¿‡ç¨‹å¯¹SFTåçš„LLMså¯¹é½èƒ½åŠ›çš„åˆå§‹æ¡ä»¶ä¸ºä»€ä¹ˆæ•æ„Ÿ)
