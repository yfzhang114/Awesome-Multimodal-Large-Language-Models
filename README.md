# Awesome-Multimodal-Large-Language-Models
his is a repository for organizing articles related to Multimodal Large Language Models, Large Language Models, and Diffusion Models; Most papers are linked to **my reading notes**. Feel free to visit my [personal homepage](https://yfzhang114.github.io/) and contact me for collaboration and discussion.


### About Me :high_brightness: 
I'm a third-year Ph.D. student at the State Key Laboratory of Pattern Recognition, the University of Chinese Academy of Sciences, advised by Prof. [Tieniu Tan](http://people.ucas.ac.cn/~tantieniu). I have also spent time at Microsoft, advised by Prof. [Jingdong Wang](https://jingdongwang2017.github.io/), alibaba DAMO Academy, work with Prof. [Rong Jin](https://scholar.google.com/citations?user=CS5uNscAAAAJ&hl=zh-CN).


###  ğŸ”¥ Updated 2024-10-28
- Recent advanced Multimodal Large Language Models and Alignment Methods have been updated.
- Our benchmark  [MME-RealWorld](https://mme-realworld.github.io/) has been released, the most difficult and largest pure manual annotation image perception benchmark so far.  [[Code]](https://github.com/yfzhang114/MME-RealWorld) [[Reading Notes]](https://zhuanlan.zhihu.com/p/717129017)
- Our model  [SliME](https://arxiv.org/abs/2406.08487) has been released, a high-resolution MLLM that can also be extend to video analysis.  [[Code]](https://github.com/yfzhang114/SliME) [[Reading Notes]](https://zhuanlan.zhihu.com/p/703258020)
- Our paper  [Debiasing Multimodal Large Language Models](https://arxiv.org/abs/2403.05262) has been released.  [[Code]](https://github.com/yfzhang114/LLaVA-Align) [[Reading Notes]](https://zhuanlan.zhihu.com/p/686461442)

# Table of Contents (ongoing)
* [Multimodal Large Language Models](#multimodal-large-language-models)
* [BenchMark and Dataset](#benchmark-and-dataset)
* [Unify Multimodal Understanding and Generation](#unify-multimodal-understanding-and-generation)
* [Alignment With Human Preference (MLLM)](#alignment-with-human-preference-mllm)
* [Alignment With Human Preference (LLM)](#alignment-with-human-preference-llm)


# Survey and Outlook
1. [ä¸‡å­—é•¿æ–‡æ€»ç»“å¤šæ¨¡æ€å¤§æ¨¡å‹æœ€æ–°è¿›å±•ï¼ˆModality Bridgingç¯‡ï¼‰](https://zhuanlan.zhihu.com/p/688215018)
2. [ä¸‡å­—é•¿æ–‡æ€»ç»“å¤šæ¨¡æ€å¤§æ¨¡å‹æœ€æ–°è¿›å±•ï¼ˆVideoç¯‡ï¼‰](https://zhuanlan.zhihu.com/p/704246896)
3. [Aligning Large Language Models with Human](https://zhuanlan.zhihu.com/p/693160839)

   
# Multimodal Large Language Models
1. [MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning](https://zhuanlan.zhihu.com/p/900354617)(appleï¼šå¤šæ¨¡æ€å¤§æ¨¡å‹ç‚¼ä¸¹æŒ‡å—)
2. [Building and better understanding vision-language models: insights and future directions](https://zhuanlan.zhihu.com/p/731680062)(Hugging Faceï¼šæ¢ç´¢å¤šæ¨¡æ€å¤§æ¨¡å‹çš„æœ€ä½³æŠ€æœ¯è·¯çº¿)
3. [Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution](https://arxiv.org/pdf/2409.12191)(ç²¾ç»†çš„åŠ¨æ€åˆ†è¾¨ç‡ç­–ç•¥+å¤šæ¨¡æ€æ—‹è½¬ä½ç½®åµŒå…¥)
4. [LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via Hybrid Architecture](https://huggingface.co/papers/2409.02889)(åœ¨å•ä¸ªA100 80GB GPUä¸Šå¯ä»¥å¤„ç†è¿‘åƒå¼ å›¾åƒ)
5. [MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?](https://zhuanlan.zhihu.com/p/717129017)(æœ€éš¾å¤šæ¨¡æ€Benchmark. QwenVL-2ç¬¬ä¸€ä½†æœªåŠæ ¼ï¼)
6. [VITA: Towards Open-Source Interactive Omni Multimodal LLM](https://zhuanlan.zhihu.com/p/714031459)(VITA : é¦–ä¸ªå¼€æºæ”¯æŒè‡ªç„¶äººæœºäº¤äº’çš„å…¨èƒ½å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹)
7. [Beyond LLaVA-HD: Diving into High-Resolution Large Multimodal Models](https://github.com/yfzhang114/SliME)(é«˜æ•ˆå¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒçš„å¤šæ¨¡æ€å¤§æ¨¡å‹)
8. [Matryoshka Multimodal Models](https://zhuanlan.zhihu.com/p/700906592)(å¦‚ä½•åœ¨æ­£ç¡®å›ç­”è§†è§‰é—®é¢˜çš„åŒæ—¶ä½¿ç”¨æœ€å°‘çš„è§†è§‰æ ‡è®°ï¼Ÿ)
9. [Chameleon: Mixed-Modal Early-Fusion Foundation Models](https://zhuanlan.zhihu.com/p/698911049)(meta: æ‰€æœ‰æ¨¡æ€éƒ½å›åˆ°token regreesionä»¥è¾¾åˆ°çµæ´»çš„ç†è§£/ç”Ÿæˆ)
10. [Flamingo: a Visual Language Model for Few-Shot Learning](https://zhuanlan.zhihu.com/p/688215018)(LLMæ¯ä¸€å±‚åˆ›å»ºé¢å¤–çš„blockå¤„ç†è§†è§‰ä¿¡æ¯)
11. [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://zhuanlan.zhihu.com/p/688215018)(q-formerèåˆè§†è§‰-è¯­è¨€ä¿¡æ¯)
12. [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://zhuanlan.zhihu.com/p/688215018)(qformer+instruction tuning)
13. [Visual Instruction Tuning](https://zhuanlan.zhihu.com/p/688215018)(MLPå¯¹é½ç‰¹å¾ï¼Œgpt4vç”Ÿæˆinstruction tuningæ•°æ®)
14. [Improved Baselines with Visual Instruction Tuning](https://zhuanlan.zhihu.com/p/688215018)(å¯¹äºllavaæ•°æ®é›†ä»¥åŠæ¨¡å‹å¤§å°çš„åˆæ­¥scaling)
15. [LLaVA-NeXT: Improved reasoning, OCR, and world knowledge](https://zhuanlan.zhihu.com/p/688215018)(åˆ†è¾¨ç‡*4ï¼Œæ•°æ®é›†æ›´å¤§)
16. [Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models](https://zhuanlan.zhihu.com/p/688215018)(ä¸€ç§ç«¯åˆ°ç«¯çš„ä¼˜åŒ–æ–¹æ¡ˆï¼Œé€šè¿‡è½»é‡çº§é€‚é…å™¨è¿æ¥å›¾åƒç¼–ç å™¨å’ŒLLM)
17. [MIMIC-IT: Multi-Modal In-Context Instruction Tuning](https://zhuanlan.zhihu.com/p/688215018)( MIMIC-ITåŒ…å«å¤šä¸ªå›¾ç‰‡æˆ–è§†é¢‘çš„è¾“å…¥æ•°æ®ï¼Œå¹¶æ”¯æŒå¤šæ¨¡æ€ä¸Šä¸‹æ–‡ä¿¡æ¯)
18. [LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding](https://zhuanlan.zhihu.com/p/688215018)(ä½¿ç”¨å…¬å¼€å¯ç”¨çš„OCRå·¥å…·åœ¨LAIONæ•°æ®é›†çš„422Kä¸ªæ–‡æœ¬ä¸°å¯Œçš„å›¾åƒä¸Šæ”¶é›†ç»“æœ)
19. [SVIT: Scaling up Visual Instruction Tuning](https://zhuanlan.zhihu.com/p/688215018)(ä¸€ä¸ªåŒ…å«420ä¸‡ä¸ªè§†è§‰æŒ‡å¯¼è°ƒæ•´æ•°æ®ç‚¹çš„æ•°æ®é›†)
20. [Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond](https://zhuanlan.zhihu.com/p/688215018)(cross attentionå¯¹é½ç‰¹å¾ï¼Œæ›´å¤§çš„ç¬¬ä¸€é˜¶æ®µè®­ç»ƒæ•°æ®)
21. [NExT-GPT: Any-to-Any Multimodal LLM](https://zhuanlan.zhihu.com/p/688215018)(ç«¯åˆ°ç«¯é€šç”¨çš„ä»»æ„å¯¹ä»»æ„MM-LLMï¼ˆMultimodal-Large Language Modelï¼‰ç³»ç»Ÿ)
22. [InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition](https://zhuanlan.zhihu.com/p/688215018)(è§†è§‰ä¿¡æ¯çš„å‹ç¼©é‡‡æ ·)
23. [CogVLM: Visual Expert for Pretrained Language Models](https://zhuanlan.zhihu.com/p/688215018)(åœ¨LLMçš„å„å±‚æ·»åŠ visual expertï¼Œå®ƒå…·æœ‰ç‹¬ç«‹çš„QKVå’ŒFFNç›¸å…³çš„å‚æ•°)
24. [OtterHD: A High-Resolution Multi-modality Model](https://zhuanlan.zhihu.com/p/688215018)(ä¸“é—¨è®¾è®¡ç”¨äºä»¥ç»†ç²’åº¦ç²¾åº¦è§£é‡Šé«˜åˆ†è¾¨ç‡è§†è§‰è¾“å…¥)
25. [Monkey : Image Resolution and Text Label Are Important Things for Large Multi-modal Models](https://zhuanlan.zhihu.com/p/688215018)(Monkeyæ¨¡å‹æå‡ºäº†ä¸€ç§æœ‰æ•ˆåœ°æé«˜è¾“å…¥åˆ†è¾¨ç‡çš„æ–¹æ³•ï¼Œæœ€é«˜å¯è¾¾ 896 x 1344 åƒç´ )
26. [LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models](https://zhuanlan.zhihu.com/p/688215018)(LLaMA-VIDèµ‹äºˆç°æœ‰æ¡†æ¶æ”¯æŒé•¿è¾¾ä¸€å°æ—¶çš„è§†é¢‘ï¼Œå¹¶é€šè¿‡é¢å¤–çš„ä¸Šä¸‹æ–‡æ ‡è®°æ¨åŠ¨äº†å®ƒä»¬çš„ä¸Šé™)
27. [MoE-LLaVA: Mixture of Experts for Large Vision-Language Models](https://zhuanlan.zhihu.com/p/688215018)(è§£å†³äº†å¤šæ¨¡æ€ç¨€ç–å­¦ä¹ ä¸­çš„æ€§èƒ½ä¸‹é™é—®é¢˜)
28. [LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images](https://zhuanlan.zhihu.com/p/688215018)(é«˜æ•ˆå¤„ç†ä»»ä½•çºµæ¨ªæ¯”å’Œé«˜åˆ†è¾¨ç‡çš„å›¾åƒ)
29. [Yi-VL](https://zhuanlan.zhihu.com/p/688215018)(Yi-VLé‡‡ç”¨äº†LLaVAæ¶æ„ï¼Œç»è¿‡å…¨é¢çš„ä¸‰é˜¶æ®µè®­ç»ƒè¿‡ç¨‹ï¼Œä»¥å°†è§†è§‰ä¿¡æ¯ä¸Yi LLMçš„è¯­ä¹‰ç©ºé—´è‰¯å¥½å¯¹é½ï¼š)
30. [Mini-Gemini](https://zhuanlan.zhihu.com/p/693063778)(åŒè§†è§‰ç¼–ç å™¨ï¼Œä½¿ç”¨ä½åˆ†è¾¨ç‡çš„è§†è§‰ç¼–ç å™¨ç‰¹å¾ä½œä¸ºqueryï¼Œå°†é«˜åˆ†è¾¨ç‡ç‰¹å¾ä½œä¸ºkey å’Œvalueè¿›è¡Œtoken mining)
31. [Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding](https://zhuanlan.zhihu.com/p/704246896)(é‡‡ç”¨äº†ä¸€ç»„åŠ¨æ€è§†è§‰tokensæ¥ç»Ÿä¸€è¡¨ç¤ºå›¾åƒå’Œè§†é¢‘ã€‚ä½¿æ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆåˆ©ç”¨æœ‰é™æ•°é‡çš„è§†è§‰tokensï¼ŒåŒæ—¶æ•æ‰å›¾åƒæ‰€éœ€çš„ç©ºé—´ç»†èŠ‚å’Œè§†é¢‘æ‰€éœ€çš„å…¨é¢æ—¶é—´å…³ç³»ã€‚)
32. [VILA: On Pre-training for Visual Language Models](https://zhuanlan.zhihu.com/p/704246896)(äº¤é”™çš„é¢„è®­ç»ƒæ•°æ®æ˜¯æœ‰ç›Šçš„ï¼Œè€Œå•çº¯çš„å›¾åƒ-æ–‡æœ¬å¯¹å¹¶éæœ€ä½³é€‰æ‹©ã€‚)
33. [ST-LLM: Large Language Models Are Effective Temporal Learners](https://zhuanlan.zhihu.com/p/704246896)(ST-LLMæå‡ºäº†ä¸€ç§åŠ¨æ€æ©ç ç­–ç•¥ï¼Œå¹¶è®¾è®¡äº†å®šåˆ¶çš„è®­ç»ƒç›®æ ‡ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹ç‰¹åˆ«é•¿çš„è§†é¢‘ï¼Œè®¾è®¡äº†ä¸€ä¸ªå…¨å±€-å±€éƒ¨è¾“å…¥æ¨¡å—ï¼Œä»¥å¹³è¡¡æ•ˆç‡å’Œæ•ˆæœã€‚)
34. [Video-LLaVA: Learning United Visual Representation by Alignment Before Projection](https://zhuanlan.zhihu.com/p/704246896)(ç”¨è§†é¢‘ç‰¹æœ‰çš„encoderæå‡è§†é¢‘ç†è§£èƒ½åŠ›è€Œéimage encoder)

# BenchMark and Dataset
1. [MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?](https://zhuanlan.zhihu.com/p/717129017)(æœ€éš¾å¤šæ¨¡æ€Benchmark. QwenVL-2ç¬¬ä¸€ä½†æœªåŠæ ¼ï¼)
2. [MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark](https://hub.baai.ac.cn/paper/baeaa2a4-4374-4cf3-927e-82df61ec3e8e)(MMMUçš„è¿›é˜¶ç‰ˆï¼Œæ›´æ³¨é‡å›¾åƒçš„æ„ŸçŸ¥å¯¹é—®é¢˜çš„å½±å“)
3. [From Pixels to Prose: A Large Dataset of Dense Image Captions](https://arxiv.org/pdf/2406.10328)(1600ä¸‡ç”Ÿæˆçš„image-text pairï¼Œåˆ©ç”¨å°–ç«¯çš„è§†è§‰è¯­è¨€æ¨¡å‹(Gemini 1.0 Pro Vision)è¿›è¡Œè¯¦ç»†å’Œå‡†ç¡®çš„æè¿°ã€‚)
4. [ShareGPT4Video: Improving Video Understanding and Generation with Better Captions](https://zhuanlan.zhihu.com/p/704246896)(40k from gpt4-v, 4814kç”Ÿæˆäºè‡ªå·±è®­ç»ƒçš„æ¨¡å‹)
5. [OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents](https://arxiv.org/pdf/2306.16527)(141 million web pages extracted from Common Crawl, 353 million associated images, and 115 billion text tokens)
6. [Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning](https://arxiv.org/abs/2306.14565)(åœ¨æ•°æ®å±‚é¢ï¼Œä»¥ç»†ç²’åº¦ç‰‡æ®µçº§æ›´æ­£çš„å½¢å¼æ”¶é›†äººç±»åé¦ˆï¼›åœ¨æ–¹æ³•å±‚é¢ï¼Œæˆ‘ä»¬æå‡ºäº†å¯†é›†ç›´æ¥åå¥½ä¼˜åŒ–(DDPO))
7. [Multimodal Self-Instruct: Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model](https://arxiv.org/abs/2407.07053)(åœ¨æ•°æ®å±‚é¢, é€šè¿‡ä»£ç ä½œä¸ºåª’ä»‹åˆæˆæŠ½è±¡å›¾è¡¨,å¹¶ä¸” benchmarking äº†å½“å‰å¤šæ¨¡æ€æ¨¡å‹åœ¨æŠ½è±¡å›¾çš„ç†è§£ä¸Šçš„ä¸è¶³.)
# Unify Multimodal Understanding and Generation

1. [Chameleon: Mixed-Modal Early-Fusion Foundation Models](https://zhuanlan.zhihu.com/p/719475102)(â€œæ—©æœŸèåˆâ€çš„æ–¹æ³•ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿè·¨æ¨¡æ€æ¨ç†å’Œç”ŸæˆçœŸæ­£çš„æ··åˆæ–‡æ¡£ã€‚)
2. [Show-o: One Single Transformer to Unify Multimodal Understanding and Generation](https://zhuanlan.zhihu.com/p/719475102)(æ–‡æœ¬ä½œä¸ºç¦»æ•£æ ‡è®°è¿›è¡Œè‡ªå›å½’å»ºæ¨¡ï¼Œè€Œè¿ç»­å›¾åƒåƒç´ åˆ™ä½¿ç”¨å»å™ªæ‰©æ•£å»ºæ¨¡ã€‚)
3. [Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model](https://zhuanlan.zhihu.com/p/719475102)(é‡‡ç”¨äº†æ–‡æœ¬çš„ä¸‹ä¸€ä¸ªæ ‡è®°é¢„æµ‹å’Œå›¾åƒçš„æ‰©æ•£ä½œä¸ºç›®æ ‡å‡½æ•°,å‹åœ¨ä¸å¢åŠ è®¡ç®—æˆæœ¬çš„å‰æä¸‹ï¼Œå®ç°äº†æ›´å¥½çš„æ¨¡æ€æ•´åˆä¸ç”Ÿæˆæ•ˆæœã€‚)
4. [VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation](https://zhuanlan.zhihu.com/p/785607305)(æ¸…å&MITï¼šç»Ÿä¸€è§†é¢‘ç†è§£ä¸ç”Ÿæˆ)
5. [MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts](https://zhuanlan.zhihu.com/p/858555788)(META:MOEæ˜¯æ··åˆæ¨¡æ€ç†è§£/ç”Ÿæˆçš„æœ€ä½³é€‰æ‹©)
6. [MIO: A Foundation Model on Multimodal Tokens](https://zhuanlan.zhihu.com/p/2186671721)(01AI: å››æ¨¡æ€ç†è§£/ç”Ÿæˆå¤§ä¸€ç»Ÿ)
7. [Harmonizing Visual Text Comprehension and Generation](https://arxiv.org/abs/2407.16364)()
# Alignment With Human Preference (MLLM)
1. [Aligning Large Multimodal Models with Factually Augmented RLHF](https://llava-rlhf.github.io/)
2. [CLIP-DPO: Vision-Language Models as a Source of Preference for Fixing Hallucinations in LVLMs](https://www.arxiv.org/abs/2408.10433)(ä½¿ç”¨é¢„è®­ç»ƒçš„ CLIP æ¨¡å‹å¯¹ LVLM è‡ªç”Ÿæˆçš„æ ‡é¢˜è¿›è¡Œæ’åºï¼Œä»¥æ„å»º DPO çš„æ­£è´Ÿå¯¹)
3. [ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models](https://www.arxiv.org/abs/2409.09318)(é€‰æ‹©äº†ä¸€ç§åŠ¨æ€ç”Ÿæˆæ–¹æ³•æ¥åˆ›å»ºä¸€ä¸ª open-set benchmarkï¼Œå¼•å…¥äº†å¼€æ”¾é›†åŠ¨æ€è¯„ä¼°åè®®(ODE)ï¼Œä¸“é—¨ç”¨äºè¯„ä¼° MLLM ä¸­çš„å¯¹è±¡å­˜åœ¨å¹»è§‰)
4. [Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization](https://arxiv.org/abs/2311.16839)(æœ¬æ–‡å°†æ¶ˆé™¤å¹»è§‰è§†ä¸ºä¸€ç§æ¨¡å‹åå¥½ï¼Œä½¿æ¨¡å‹åå‘äºæ— å¹»è§‰è¾“å‡ºï¼Œäºæ˜¯æå‡ºäº†ä¸€ç§å¯¹å¹»è§‰æ•æ„Ÿçš„å¤šæ¨¡æ€DPO ç­–ç•¥ â€”â€” HA-DPOã€‚æˆ‘ä»¬è¿˜å¼•å…¥äº†å¥å­çº§å¹»è§‰æ¯”ç‡(SHR)ï¼Œå®ƒä¸å—å›ºå®šç±»åˆ«å’ŒèŒƒå›´çš„é™åˆ¶ï¼Œä¸ºå¤šæ¨¡æ€å¹»è§‰æä¾›äº†å¹¿æ³›ã€ç»†ç²’åº¦å’Œå®šé‡çš„æµ‹é‡)
5. [Detecting and Preventing Hallucinations in Large Vision Language Models](https://arxiv.org/abs/2308.06394)(ä¸ºäº†ä¾¿äºè‡ªåŠ¨æ£€æµ‹å¹»è§‰ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨ InstructBLIP çš„ VQA å“åº”æ„å»ºäº†ä¸€ä¸ªå¤šæ ·åŒ–çš„äººå·¥æ ‡è®°æ•°æ®é›† M-HalDetectï¼Œä¸“æ³¨äºåœ¨è¯¦ç»†å›¾åƒæè¿°çš„å­å¥çº§åˆ«ä¸Šè¿›è¡Œç»†ç²’åº¦æ³¨é‡Šã€‚åœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒä¸åŒå¯†åº¦(å¥å­çº§ï¼Œå­å¥å­çº§)çš„å¤šä¸ªå¥–åŠ±æ¨¡å‹ï¼Œç”¨äºå¹»è§‰æ£€æµ‹ã€‚æˆ‘ä»¬ä¹Ÿä½¿ç”¨ç»†ç²’åº¦ç›´æ¥åå¥½ä¼˜åŒ–(FDPO)ç›´æ¥ä¼˜åŒ– InstructBLIP)
6. [RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthiness](https://arxiv.org/abs/2405.17220)(åŒä¸€ä¸ªå¤§æ¨¡å‹ç”Ÿæˆå¤šä¸ªå›å¤ï¼Œå°†å›å¤æŒ‰å¥æ‹†åˆ†ï¼Œä¹‹åè½¬åŒ–ä¸ºé—®å¥è®©å¼€æºæ¨¡å‹å›å¤å‡†ç¡®åº¦ï¼Œå°†æ‰€æœ‰å‡†ç¡®åº¦ç›¸åŠ ï¼Œå¾—åˆ°åå¥½æ•°æ®ï¼Œç”¨äºè¿­ä»£DPO)
7. [Enhancing Visual-Language Modality Alignment in Large Vision Language Models via Self-Improvement](https://arxiv.org/abs/2405.15973)(æˆ‘ä»¬æå‡ºäº† Self-Improvement Modality Alignment(SIMA)ï¼Œæ—¨åœ¨é€šè¿‡è‡ªæˆ‘å®Œå–„æœºåˆ¶è¿›ä¸€æ­¥æ”¹å–„ LVLM å†…è§†è§‰æ¨¡æ€å’Œè¯­è¨€æ¨¡æ€ä¹‹é—´çš„å¯¹é½)

# Alignment With Human Preference (LLM)

1. [ChatGLM-Mathï¼šImproving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline](https://zhuanlan.zhihu.com/p/698983475)(ChatGLM-Math: Self-Critiqueè¿­ä»£å¯¹é½æ˜¾è‘—æå‡æ•°å­¦èƒ½åŠ›)
2. [Beyond One-Preference-Fits-All Alignment: Multi-Objective Direct Preference Optimization](https://zhuanlan.zhihu.com/p/698782623)(å¤§è¯­è¨€æ¨¡å‹çš„å¤šç›®æ ‡å¯¹é½)
3. [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://zhuanlan.zhihu.com/p/693163438)(ç›´æ¥åå¥½ä¼˜åŒ–å…‹æœRLHFä¸ç¨³å®šçš„é—®é¢˜)
4. [KTO: Model Alignment as Prospect Theoretic Optimization](https://zhuanlan.zhihu.com/p/693163438)(ä¸éœ€è¦æˆå¯¹æ•°æ®çš„åå¥½ä¼˜åŒ–)
5. [Direct Preference Optimization with an Offset](https://zhuanlan.zhihu.com/p/693163438)(å¸¦åç§»çš„DPO, è¦æ±‚é¦–é€‰å“åº”å’Œä¸å—æ¬¢è¿å“åº”ä¹‹é—´çš„å¯èƒ½æ€§å·®å¼‚å¤§äºä¸€ä¸ªåç§»å€¼)
6. [Contrastive preference learning: Learning from human feedback without reinforcement learning](https://zhuanlan.zhihu.com/p/693163438)(å¯¹æ¯”åå¥½å­¦ä¹ ï¼ˆCPLï¼‰ç®—æ³•ï¼Œè¯¥ç®—æ³•ç”¨äºä»åå¥½ä¸­å­¦ä¹ æœ€ä¼˜ç­–ç•¥è€Œæ— éœ€å­¦ä¹ å¥–åŠ±å‡½æ•°ï¼Œä»è€Œé¿å…äº†å¯¹RLçš„éœ€æ±‚)
7. [Statistical Rejection Sampling Improves Preference Optimization](https://zhuanlan.zhihu.com/p/693163438)(ä½¿ç”¨æ‹’ç»æŠ½æ ·ä»ç›®æ ‡æœ€ä¼˜ç­–ç•¥ä¸­è·å–åå¥½æ•°æ®ï¼Œä»è€Œæ›´å‡†ç¡®åœ°ä¼°è®¡æœ€ä¼˜ç­–ç•¥)
8. [Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study](https://zhuanlan.zhihu.com/p/693163438)(åœ¨æ‰€æœ‰å®éªŒä¸­ï¼ŒPPOå§‹ç»ˆä¼˜äºDPOã€‚ç‰¹åˆ«æ˜¯åœ¨æœ€å…·æŒ‘æˆ˜æ€§çš„ä»£ç ç«èµ›ä»»åŠ¡ä¸­ï¼ŒPPOå®ç°äº†æœ€å…ˆè¿›çš„ç»“æœ)
9. [Fine-tuning Aligned Language Models Compromises Safety](https://zhuanlan.zhihu.com/p/696707347)(å¾®è°ƒå¯¹é½çš„è¯­è¨€æ¨¡å‹ä¼šæŸå®³å®‰å…¨æ€§)
10. [ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline](https://mp.weixin.qq.com/s/lg7ueR9b-om0ecUEoT4x8w)(reward model, Rejective Fine-tuning, then DPOè¿­ä»£æå‡æ¨¡å‹æ•°å­¦æ€§èƒ½)
11. [SimPO: Simple Preference Optimization with a Reference-Free Reward](https://zhuanlan.zhihu.com/p/700438956)(length reg+å»æ‰ref model)
12. [towards analyzing and understanding the limitations of dpo: a theoretical perspective](https://zhuanlan.zhihu.com/p/701213691)(DPOçš„å®é™…ä¼˜åŒ–è¿‡ç¨‹å¯¹SFTåçš„LLMså¯¹é½èƒ½åŠ›çš„åˆå§‹æ¡ä»¶ä¸ºä»€ä¹ˆæ•æ„Ÿ)
13. [Iterative Length-Regularized Direct Preference Optimization: A Case Study on Improving 7B Language Models to GPT-4 Level](https://arxiv.org/abs/2406.11817)(è¡¨æ˜è¿­ä»£ DPO (iDPO)å¯ä»¥é€šè¿‡ç²¾å¿ƒè®¾è®¡å°† 7B æ¨¡å‹çš„ LC win rate å¢å¼ºåˆ° GPT-4 æ°´å¹³)
14. [Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs](https://arxiv.org/abs/2406.18629)(å‡ºäº†ä¸€ç§æœ‰æ•ˆä¸”ç»æµçš„ pipeline æ¥æ”¶é›†æˆå¯¹æ•°å­¦é—®é¢˜åå¥½æ•°æ®ã€‚å¼•å…¥äº† Step-DPOï¼Œæœ€å¤§åŒ–ä¸‹ä¸€ä¸ªæ¨ç†æ­¥éª¤æ­£ç¡®çš„æ¦‚ç‡ï¼Œæœ€å°åŒ–å…¶é”™è¯¯çš„æ¦‚ç‡)
